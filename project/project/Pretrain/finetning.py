# -*- coding: utf-8 -*-
"""FineTning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OuOvxDnG4208VKL98_8XUXpYfa2f4C1b

FineTning English
"""

# Importing necessary libraries
import pandas as pd
import numpy as np
import torch
from transformers import BertTokenizer, BertModel
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
import shap
import tensorflow as tf
from sklearn.metrics import classification_report

# Load and preprocess the data
data = pd.read_csv('/content/essays_preprocessing.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)
data.head()

# Store the original text data in a separate variable
original_texts = data['text'].tolist()

# Load pre-trained BERT model and tokenizer
model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
bert_model = BertModel.from_pretrained(model_name)  # Renamed from `model` to `bert_model`
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
bert_model.to(device)

# Define a function to tokenize and encode the text data
def get_bert_token_embeddings(texts):
    input_ids = []
    attention_masks = []
    tokenized_texts = []

    for text in texts:
        encoded_text = tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=256,  # Truncate the sequence to 256 tokens to save memory
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )
        input_ids.append(encoded_text['input_ids'])
        attention_masks.append(encoded_text['attention_mask'])

        # Save the tokenized version of the text
        tokens = tokenizer.convert_ids_to_tokens(encoded_text['input_ids'].flatten())
        tokenized_texts.append(tokens)

    input_ids = torch.cat(input_ids, dim=0)
    attention_masks = torch.cat(attention_masks, dim=0)

    with torch.no_grad():
        embeddings = bert_model(input_ids.to(device), attention_mask=attention_masks.to(device))

    # Return the embeddings for all tokens, not just the CLS token
    return embeddings[0].cpu().numpy(), tokenized_texts

# Define a function to perform average pooling on the token embeddings
def average_pooling(embeddings):
    # Take the mean of the token embeddings across the sequence length (axis 1)
    return np.mean(embeddings, axis=1)

# Adjust batch size to prevent memory overload
batch_size = 32  # Reduce the batch size to save memory
num_batches = (len(original_texts) - 1) // batch_size + 1

embeddings = []
token_list = []  # Store token lists
for i in range(num_batches):
    start = i * batch_size
    end = min((i + 1) * batch_size, len(original_texts))  # Adjust the end index for the last batch
    batch_texts = original_texts[start:end]

    try:
        batch_embeddings, batch_tokens = get_bert_token_embeddings(batch_texts)
        # Perform average pooling to reduce the token embeddings to a 2D array
        batch_embeddings_avg = average_pooling(batch_embeddings)
        embeddings.append(batch_embeddings_avg)
        token_list.extend(batch_tokens)  # Collect the tokens for the batch
    except RuntimeError as e:
        if 'CUDA out of memory' in str(e):
            # Handle the out-of-memory error gracefully
            print("CUDA out of memory error occurred. Skipping batch {}.".format(i))
            torch.cuda.empty_cache()  # Clear the cache to free memory
        else:
            print("Runtime error occurred:", e)

# Combine all the embeddings into one array  token_list
embeddings = np.concatenate(embeddings, axis=0)

# Save the BERT embeddings
joblib.dump(embeddings, 'bert_embeddings_essays.pkl')

"""Fine-Tuning Arabic"""

import torch
from transformers import BertTokenizer, BertModel
import numpy as np
import pandas as pd
import joblib
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

# Load and preprocess the data
data = pd.read_csv('/content/AraPersonality_with_text.csv')
print(data.dropna(inplace=True))
print("Data shape:", data.shape)
data.head()

# Load and preprocess your Arabic text data
texts = data['text'][0:92]  # List of texts (in Arabic)
# labels = data['NS'].values[0:3000]  # List of corresponding labels (optional)

# Load pre-trained AraBERT model and tokenizer
model_name = 'aubmindlab/bert-base-arabertv02'  # Specify the AraBERT model
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Define a function to tokenize and encode the Arabic text data
def get_bert_embeddings(texts):
    input_ids = []
    attention_masks = []

    for text in texts:
        encoded_text = tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=512,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )

        input_ids.append(encoded_text['input_ids'])
        attention_masks.append(encoded_text['attention_mask'])

    input_ids = torch.cat(input_ids, dim=0)
    attention_masks = torch.cat(attention_masks, dim=0)

    with torch.no_grad():
        embeddings = model(input_ids.to(device), attention_mask=attention_masks.to(device))

    return embeddings[0][:, 0, :].cpu().numpy()

# Split the data into smaller batches for processing
batch_size = 32
num_batches = (len(texts) - 1) // batch_size + 1

embeddings = []
for i in range(num_batches):
    start = i * batch_size
    end = min((i + 1) * batch_size, len(texts))  # Adjust the end index for the last batch
    batch_texts = texts[start:end]

    try:
        batch_embeddings = get_bert_embeddings(batch_texts)
        embeddings.append(batch_embeddings)
    except RuntimeError as e:
        if 'CUDA out of memory' in str(e):
            # Handle the out-of-memory error gracefully
            print("CUDA out of memory error occurred. Skipping batch {}.".format(i))
            # Additional error handling or memory optimization can be performed here
        else:
            # Handle other runtime errors
            print("Runtime error occurred:", e)
            # Additional error handling can be performed here

embeddings = np.concatenate(embeddings, axis=0)

# Save the BERT embeddings
joblib.dump(embeddings, 'bert_embeddings_arabic1.pkl')