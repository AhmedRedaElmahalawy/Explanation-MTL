# -*- coding: utf-8 -*-
"""run_model_Combine.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SkULegFsAT_jmNVMG6pOdv5ktM7rp6tv

Libraries
"""

import numpy as np
import pandas as pd
import joblib
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.optim.lr_scheduler import OneCycleLR
import tensorflow as tf
import shap
import matplotlib.pyplot as plt
from scipy.stats import wasserstein_distance
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, precision_recall_curve
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from keras import Sequential, Model
from keras.metrics import Accuracy
from keras.layers import Dense, BatchNormalization, Dropout, ReLU, Conv1D, MaxPooling1D, Flatten, Reshape
from keras.layers import LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D
from keras.losses import Loss, binary_crossentropy
R1 = 42

"""CNN"""

R = 62
np.random.seed(R)
tf.random.set_seed(R )

# Load and preprocess the data
data = pd.read_csv('/content/project/project/dataset/Fused-Arabic/dataset_Combined_Arabic.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

# Extract text data and labels
texts = data['text']
labels = data[['E', 'N', 'A', 'C', 'O']]

# Load the saved BERT embeddings
embeddings = joblib.load('/content/project/project/dataset/Fused-Arabic/bert_embeddings_Merge_arabic.pkl')
X = np.array(embeddings)

# Define labels for each task
y1 = labels['E']
y2 = labels['N']
y3 = labels['A']
y4 = labels['C']
y5 = labels['O']

# Combine labels into a single array for multi-task learning
y = np.column_stack((y1, y2, y3, y4, y5))

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=R1)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=R1)

# Loss Function
class MultiTaskBCE(Loss):
    def __init__(self, num_tasks: int, task_weights=0.02) -> None:
        super().__init__()

        if task_weights is None:
            self.task_weights = tf.ones((1, num_tasks))
        elif tf.rank(task_weights) == 1:
            self.task_weights = tf.expand_dims(task_weights, 0)
        else:
            self.task_weights = task_weights

    def call(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:
        bce = binary_crossentropy(y_true, y_pred)  # (bs, num_tasks)
        loss = self.task_weights * tf.reduce_mean(bce, axis=0)  # (1, num_tasks)
        loss = tf.reduce_sum(loss)  # (1)

        return loss

# CNN Model
class CNN(Sequential):
    def __init__(self, num_conv_layers: int, filters: int, kernel_size: int, pool_size: int, dense_units: int, dim_out: int = None, dropout: float = 0.0, name: str = "CNN"):
        layers = []

        # Reshape input to add a channel dimension
        layers.append(Reshape((-1, 1)))  # Adding channel dimension

        for _ in range(num_conv_layers):
            layers.append(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu'))
            layers.append(MaxPooling1D(pool_size=pool_size))
            layers.append(BatchNormalization())

        layers.append(Flatten())

        if dense_units:
            layers.append(Dense(dense_units, activation='relu'))

        if dropout > 0.0:
            layers.append(Dropout(dropout))

        if dim_out:
            layers.append(Dense(dim_out))

        super().__init__(layers, name=name)

# MGMOE Model
class MultiGateMixtureOfExperts(Model):
    def __init__(self, num_tasks: int, num_emb: int, dim_emb: int = 32, embedding_l2: float = 0.0, num_experts: int = 1,
                 num_conv_layers_expert: int = 2, filters_expert: int = 64, kernel_size_expert: int = 3, pool_size_expert: int = 2, dense_units_expert: int = 64, dropout_expert: float = 0.0,
                 gate_function: str = "softmax", num_conv_layers_tasks: int = 2, filters_tasks: int = 64, kernel_size_tasks: int = 3, pool_size_tasks: int = 2, dense_units_tasks: int = 64,
                 dim_out_tasks: int = 1, dropout_tasks: float = 0.0):
        super().__init__()

        # Experts
        self.experts = []
        for _ in range(num_experts):
            self.experts.append(CNN(num_conv_layers_expert, filters_expert, kernel_size_expert, pool_size_expert, dense_units_expert, dropout=dropout_expert))

        # Towers and Gates for each task
        self.towers = []
        self.gates = []
        for _ in range(num_tasks):
            self.towers.append(CNN(num_conv_layers_tasks, filters_tasks, kernel_size_tasks, pool_size_tasks, dense_units_tasks, dim_out_tasks, dropout=dropout_tasks))
            self.gates.append(Dense(num_experts, activation=gate_function, use_bias=False))

    def call(self, inputs: tf.Tensor, training: bool = None) -> tf.Tensor:
        # Experts output
        out_experts = []
        for expert in self.experts:
            expert_input = tf.expand_dims(inputs, -1)  # Add channel dimension (None, 768) -> (None, 768, 1)
            out_experts.append(expert(expert_input, training=training))  # (bs, num_hidden_expert)

        out_experts = tf.stack(out_experts, axis=-1)  # (bs, num_hidden_expert, num_experts)

        # Task-specific outputs
        out_tasks = []
        for gate, tower in zip(self.gates, self.towers):
            gate_score = gate(inputs, training=training)  # (bs, num_experts)
            in_task = tf.einsum("bie,be->bi", out_experts, gate_score)  # (bs, num_hidden_expert)
            logits_task = tower(tf.expand_dims(in_task, -1), training=training)  # (bs, 1)
            out_tasks.append(logits_task)

        out = tf.concat(out_tasks, -1)  # (bs, num_tasks)
        out = tf.nn.sigmoid(out)  # (bs, num_tasks)

        return out  # (bs, num_tasks)

# Create and compile the model
num_tasks = 5
num_embeddings = X_train.shape[1]  # Assuming that the embeddings are of shape (samples, embedding_size)

model = MultiGateMixtureOfExperts(
    num_tasks=num_tasks,
    num_emb=num_embeddings,
    dim_emb=128,
    num_experts=3,
    num_conv_layers_expert=3,
    filters_expert=64,
    kernel_size_expert=3,
    pool_size_expert=2,
    dense_units_expert=64,
    dropout_expert=0.1,
    gate_function="softmax",
    num_conv_layers_tasks=2,
    filters_tasks=64,
    kernel_size_tasks=3,
    pool_size_tasks=2,
    dense_units_tasks=64,
    dim_out_tasks=1,
    dropout_tasks=0.1,
)

loss = MultiTaskBCE(num_tasks=num_tasks)
optimizer = tf.keras.optimizers.Adam()

model.compile(optimizer=optimizer, loss=loss, metrics=[Accuracy()])

# Train the model
history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)

# Evaluate the model
y_pred = model.predict(X_test)
y_pred_labels = (y_pred > 0.5).astype(int)

lambda_f = accuracy_score(y_test, y_pred_labels)
# SHAP KernelExplainer Integration
def model_predict(X_np):
    X_tensor = tf.convert_to_tensor(X_np, dtype=tf.float32)
    y_pred = model(X_tensor, training=False).numpy()
    return y_pred

# Create SHAP explainer object for KernelExplainer
explainer = shap.KernelExplainer(model_predict, X_train[:4])  # Use a small subset of the training data for background
shap_values = explainer.shap_values(X_test[:4])  # Explain the first 10 samples from the test set

# Visualize SHAP values
# shap.summary_plot(shap_values, X_test[:4])  # Visualize SHAP values for the test set
# Compute Wasserstein distance loss
def wasserstein_loss(shap_values, reference_values):
    shap_values = np.array(shap_values)
    reference_values = np.array(reference_values)
    return np.mean([wasserstein_distance(shap_values[i].flatten(), reference_values[i].flatten())
                    for i in range(shap_values.shape[0])])

L_exp = wasserstein_loss(shap_values, np.mean(shap_values, axis=1))

# Compute the Joint Training Loss
L_joint = (1 - lambda_f) * history.history['loss'][-1] + lambda_f * L_exp

# CNN Expert
class CNNExpert(nn.Module):
    def __init__(self, input_dim, num_conv_layers=2, filters=64, kernel_size=3, pool_size=2, dense_units=64, dropout=0.5):
        super(CNNExpert, self).__init__()
        layers = []

        self.input_dim = input_dim
        self.filters = filters

        # Convolutional layers
        for i in range(num_conv_layers):
            in_channels = 1 if i == 0 else filters  # First layer has 1 channel
            layers.append(nn.Conv1d(in_channels=in_channels, out_channels=filters, kernel_size=kernel_size))
            layers.append(nn.MaxPool1d(kernel_size=pool_size))
            layers.append(nn.BatchNorm1d(filters))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout))

        # Add the convolution layers to the module
        self.conv_net = nn.Sequential(*layers)

        # Compute the output size after the convolutional layers
        dummy_input = torch.zeros(1, 1, input_dim)  # Batch size of 1, 1 channel, and input_dim features
        conv_out = self.conv_net(dummy_input)
        self.flattened_size = conv_out.numel()  # Get the total number of features after flattening

        # Fully connected layers
        self.fc = nn.Linear(self.flattened_size, dense_units)
        self.output_layer = nn.Linear(dense_units, 1)

    def forward(self, x):
        # Add channel dimension: (batch_size, input_dim) -> (batch_size, 1, input_dim)
        x = x.unsqueeze(1)  # Add a channel dimension using unsqueeze
        # Pass through the convolutional layers
        x = self.conv_net(x)

        # Flatten the output from conv layers
        x = x.view(x.size(0), -1)  # Flatten the output to (batch_size, flattened_size)

        # Pass through the fully connected layers
        x = torch.relu(self.fc(x))
        x = torch.sigmoid(self.output_layer(x))  # Final output
        return x.squeeze()  # Remove the extra dimension for binary classification

# MGMOE Model
class MultiGateMixtureOfExperts(nn.Module):
    def __init__(self, input_dim, num_tasks, num_experts, ExpertNet):
        super(MultiGateMixtureOfExperts, self).__init__()
        self.num_tasks = num_tasks
        self.num_experts = num_experts
        self.experts = nn.ModuleList([ExpertNet(input_dim) for _ in range(num_experts)])
        self.gates = nn.ParameterList([nn.Parameter(torch.randn(input_dim, num_experts)) for _ in range(num_tasks)])

    def forward(self, inputs):
        expert_outputs = torch.stack([expert(inputs) for expert in self.experts], dim=-1)
        task_outputs = []
        for gate in self.gates:
            gate_weights = torch.softmax(inputs @ gate, dim=-1)
            task_output = torch.sum(expert_outputs * gate_weights, dim=-1)
            task_outputs.append(task_output)
        return torch.stack(task_outputs, dim=-1)
# # Set random seed for reproducibility
# np.random.seed(R)
# torch.manual_seed(R)

# Load and preprocess the data
data = pd.read_csv('/content/project/project/dataset/Fused-Arabic/dataset_Combined_Arabic.csv')
data.dropna(inplace=True)
# Extract labels
labels = data[['E', 'N', 'A', 'C', 'O']]
embeddings = joblib.load('/content/project/project/dataset/Fused-Arabic/bert_embeddings_Merge_arabic.pkl')
X = np.array(embeddings)

y = np.column_stack([labels[col] for col in labels.columns])
y = np.clip(y, 0, 1).astype(np.float32)

# Train/val/test split
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=R)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.331, random_state=R)
# 33
X_train = torch.tensor(X_train, dtype=torch.float32)
X_val = torch.tensor(X_val, dtype=torch.float32)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32)
y_val = torch.tensor(y_val, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.float32)
# Hyperparameters
input_dim = X_train.shape[1]
num_tasks = 5
num_experts =3
batch_size = 126
epochs = 30

model = MultiGateMixtureOfExperts(input_dim, num_tasks, num_experts, CNNExpert)
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
scheduler = OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(X_train) // batch_size, epochs=epochs)
loss_fn = nn.BCELoss()  # Use BCE loss for now

# EarlyStopping
class EarlyStopping:
    def __init__(self, patience=10, verbose=True, delta=0.0001):
        self.patience = patience
        self.verbose = verbose
        self.delta = delta
        self.best_score = None
        self.epochs_no_improve = 0.0001
        self.early_stop = False

    def __call__(self, val_loss, model):
        score = -val_loss
        if self.best_score is None:
            self.best_score = score
        elif score < self.best_score + self.delta:
            self.epochs_no_improve += 1
            if self.epochs_no_improve >= self.patience:
                if self.verbose:
                    print("Early stopping")
                self.early_stop = True
        else:
            self.best_score = score
            self.epochs_no_improve = 0

# Validation function
def validate_model(model, X_val, y_val, loss_fn):
    model.eval()
    with torch.no_grad():
        outputs = model(X_val)
        loss = loss_fn(outputs, y_val)
    return loss.item()

# Training
def train_model(model, X_train, y_train, X_val, y_val, loss_fn, optimizer, batch_size, epochs, early_stopping, scheduler):
    model.train()
    num_batches = len(X_train) // batch_size
    for epoch in range(epochs):
        epoch_loss = L_joint   #L_joint
        for i in range(num_batches):
            batch_x = X_train[i * batch_size:(i + 1) * batch_size]
            batch_y = y_train[i * batch_size:(i + 1) * batch_size]
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = loss_fn(outputs, batch_y)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            scheduler.step()
            epoch_loss += loss.item()
        val_loss = validate_model(model, X_val, y_val, loss_fn)
        # print(f"Epoch [{epoch+1}/{epochs}] Loss: {epoch_loss / num_batches:.4f}, Val Loss: {val_loss:.4f}, CNN: {scheduler.get_last_lr()[0]:.6f}")
        early_stopping(val_loss, model)
        if early_stopping.early_stop:
            break

early_stopping = EarlyStopping(patience=10, verbose=True)
train_model(model, X_train, y_train, X_val, y_val, loss_fn, optimizer, batch_size, epochs, early_stopping, scheduler)

# Threshold optimization
def calculate_optimal_thresholds(y_true, y_scores):
    optimal_thresholds = []
    for i in range(y_true.shape[1]):
        precision, recall, thresholds = precision_recall_curve(y_true[:, i], y_scores[:, i])
        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)
        optimal_idx = np.nanargmax(f1_scores)
        best_threshold = thresholds[optimal_idx] if optimal_idx < len(thresholds) else 0.5
        optimal_thresholds.append(best_threshold)
        # print(f"Class {i}: Best Threshold = {best_threshold:.4f}, Max F1 = {f1_scores[optimal_idx]:.4f}")
    return optimal_thresholds

# Final evaluation
def evaluate_model_with_optimal_thresholds(model, X_test, y_test, optimal_thresholds):
    model.eval()
    with torch.no_grad():
        y_scores = model(X_test).numpy()
        # print("Sample prediction probabilities:", y_scores[0])
        y_pred_labels = np.zeros_like(y_scores)
        for i, threshold in enumerate(optimal_thresholds):
            y_pred_labels[:, i] = (y_scores[:, i] > threshold).astype(int)
        f1_micro = f1_score(y_test.numpy(), y_pred_labels, average='micro')
        f1_macro = f1_score(y_test.numpy(), y_pred_labels, average='macro')
        print(f"CNN Results (%)")
        print(f"F1-micro: {f1_micro:.4f}")
        print(f"F1-macro: {f1_macro:.4f}")

# Run final eval
y_scores_val = model(X_val).detach().numpy()
optimal_thresholds = calculate_optimal_thresholds(y_val.numpy(), y_scores_val)
evaluate_model_with_optimal_thresholds(model, X_test, y_test, optimal_thresholds)

"""MLP"""

# Set random seed for reproducibility
R = 62
np.random.seed(R)
torch.manual_seed(R)

# Load and preprocess the data
data = pd.read_csv('/content/project/project/dataset/Fused-Arabic/dataset_Combined_Arabic.csv')
data.dropna(inplace=True)
# Extract text data and labels
texts = data['text']
labels = data[['E', 'N', 'A', 'C', 'O']]

# Load the saved BERT embeddings
embeddings = joblib.load('/content/project/project/dataset/Fused-Arabic/bert_embeddings_Merge_arabic.pkl')
X = np.array(embeddings)

# Define labels for each task
y = np.column_stack([labels[col] for col in ['E', 'N', 'A', 'C', 'O']])

# Ensure the labels are binary (0 or 1) and convert to float tensors
y = np.clip(y, 0, 1)
y = y.astype(np.float32)

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=R1)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=R1)

# Convert data to PyTorch tensors
X_train = torch.tensor(X_train, dtype=torch.float32)
X_val = torch.tensor(X_val, dtype=torch.float32)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32)
y_val = torch.tensor(y_val, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.float32)

# Define PyTorch MLP Model (replacing Logistic Regression)
class MLP(nn.Module):
    def __init__(self, input_dim, num_hidden=2, dim_hidden=64, dim_out=1, dropout=0.1):
        super(MLP, self).__init__()
        layers = []
        current_dim = input_dim
        for _ in range(num_hidden - 1):
            layers.append(nn.Linear(current_dim, dim_hidden))
            layers.append(nn.BatchNorm1d(dim_hidden))
            layers.append(nn.ReLU())
            if dropout > 0.0:
                layers.append(nn.Dropout(dropout))
            current_dim = dim_hidden

        layers.append(nn.Linear(current_dim, dim_out))
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        return torch.sigmoid(self.net(x)).squeeze()

# MGMOE Model with MLP Experts using PyTorch
class MultiGateMixtureOfExperts(nn.Module):
    def __init__(self, input_dim, num_tasks, num_experts, ExpertNet):
        super(MultiGateMixtureOfExperts, self).__init__()
        self.num_tasks = num_tasks
        self.num_experts = num_experts
        self.experts = nn.ModuleList([ExpertNet(input_dim) for _ in range(num_experts)])
        self.gates = nn.ParameterList([nn.Parameter(torch.randn(input_dim, num_experts)) for _ in range(num_tasks)])

    def forward(self, inputs):
        out_experts = torch.stack([expert(inputs) for expert in self.experts], dim=-1)
        out_tasks = []
        for gate in self.gates:
            gate_scores = torch.softmax(inputs @ gate, dim=-1)
            task_output = torch.sum(out_experts * gate_scores, dim=-1)
            out_tasks.append(task_output)
        out = torch.stack(out_tasks, dim=-1)
        return torch.sigmoid(out)

# Define Mean Squared Error (MSE) Loss Function
def loss(y_true, y_pred):
    criterion = torch.nn.MSELoss()
    return criterion(y_pred, y_true)

# Hyperparameters
input_dim = X_train.shape[1]
num_tasks = 5
num_experts = 3
learning_rate = 0.001
epochs = 30
batch_size = 32

# Instantiate the model and optimizer
model = MultiGateMixtureOfExperts(
    input_dim=input_dim,
    num_tasks=num_tasks,
    num_experts=num_experts,
    ExpertNet=lambda input_dim: MLP(input_dim, num_hidden=2, dim_hidden=64, dim_out=1, dropout=0.1)
)
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
def train_model(model, X_train, y_train, optimizer, batch_size, epochs):
    global L_label
    model.train()
    num_batches = len(X_train) // batch_size
    for epoch in range(epochs):
        epoch_loss = 0
        for i in range(num_batches):
            batch_x = X_train[i*batch_size:(i+1)*batch_size]
            batch_y = y_train[i*batch_size:(i+1)*batch_size]
            outputs = model(batch_x)
            loss_value = loss(batch_y, outputs)
            epoch_loss += loss_value.item()
            optimizer.zero_grad()
            loss_value.backward()
            optimizer.step()
        L_label = epoch_loss / num_batches  # Store last epoch loss
        # print(f'Epoch [{epoch+1}/{epochs}], Loss: {L_label:.4f}')

# Train the model
train_model(model, X_train, y_train, optimizer, batch_size, epochs)

# Evaluate the model
model.eval()
with torch.no_grad():
    y_pred = model(X_test).numpy()
    y_pred_labels = (y_pred > 0.5).astype(int)

# Calculate accuracy for λ_f
lambda_f = accuracy_score(y_test.numpy(), y_pred_labels)
# --------------------------------------------
# SHAP KernelExplainer Integration
# --------------------------------------------

# Define a prediction function for SHAP
def model_predict(X_np):
    X_tensor = torch.tensor(X_np, dtype=torch.float32)
    with torch.no_grad():
        return model(X_tensor).numpy()

# Select a small subset of the training data for SHAP KernelExplainer reference
X_sample = X_train[:50].numpy()
X_explain = X_test[:10].numpy()

# Apply SHAP KernelExplainer
explainer = shap.KernelExplainer(model_predict, X_sample)
shap_values = explainer.shap_values(X_explain)

# Visualize SHAP values
# shap.summary_plot(shap_values, X_explain)
# Compute Wasserstein distance loss
def wasserstein_loss(shap_values, reference_values):
    shap_values = np.array(shap_values)
    reference_values = np.array(reference_values)
    return np.mean([wasserstein_distance(shap_values[i].flatten(), reference_values[i].flatten())
                    for i in range(shap_values.shape[0])])

L_exp = wasserstein_loss(shap_values, np.mean(shap_values, axis=1))

# Compute the Joint Training Loss
L_joint = (1 - lambda_f) * L_label + lambda_f * L_exp

# MLP Expert
class MLPExpert(nn.Module):
    def __init__(self, input_dim):
        super(MLPExpert, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, input_dim // 2),
            nn.BatchNorm1d(input_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.8),
            nn.Linear(input_dim // 2, 1)
        )

    def forward(self, x):
        return torch.sigmoid(self.net(x)).squeeze()

# MGMOE
class MultiGateMixtureOfExperts(nn.Module):
    def __init__(self, input_dim, num_tasks, num_experts, ExpertNet):
        super(MultiGateMixtureOfExperts, self).__init__()
        self.num_tasks = num_tasks
        self.num_experts = num_experts
        self.experts = nn.ModuleList([ExpertNet(input_dim) for _ in range(num_experts)])
        self.gates = nn.ParameterList([
            nn.Parameter(torch.randn(input_dim, num_experts)) for _ in range(num_tasks)
        ])

    def forward(self, inputs):
        expert_outputs = torch.stack([expert(inputs) for expert in self.experts], dim=-1)
        task_outputs = []
        for gate in self.gates:
            gate_weights = torch.softmax(inputs @ gate, dim=-1)
            task_output = torch.sum(expert_outputs * gate_weights, dim=-1)
            task_outputs.append(task_output)
        return torch.stack(task_outputs, dim=-1)
np.random.seed(R)
torch.manual_seed(R)

# Load data
data = pd.read_csv('/content/project/project/dataset/Fused-Arabic/dataset_Combined_Arabic.csv')
data.dropna(inplace=True)
labels = data[['E', 'N', 'A', 'C', 'O']]
embeddings = joblib.load('/content/project/project/dataset/Fused-Arabic/bert_embeddings_Merge_arabic.pkl')
X = np.array(embeddings)
y = np.clip(labels.to_numpy(), 0, 1).astype(np.float32)

# Split
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=R)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=R)

X_train = torch.tensor(X_train, dtype=torch.float32)
X_val = torch.tensor(X_val, dtype=torch.float32)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32)
y_val = torch.tensor(y_val, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.float32)
# Hyperparams
input_dim = X_train.shape[1]
num_tasks = 5
num_experts = 3
batch_size = 64#32
epochs = 30

model = MultiGateMixtureOfExperts(input_dim, num_tasks, num_experts, MLPExpert)
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
scheduler = OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(X_train) // batch_size, epochs=epochs)
loss_fn = nn.BCELoss()

class EarlyStopping:
    def __init__(self, patience=10, verbose=True, delta=0.0001):
        self.patience = patience
        self.verbose = verbose
        self.delta = delta
        self.best_score = None
        self.epochs_no_improve = 0
        self.early_stop = False

    def __call__(self, val_loss, model):
        score = -val_loss
        if self.best_score is None:
            self.best_score = score
        elif score < self.best_score + self.delta:
            self.epochs_no_improve += 1
            if self.epochs_no_improve >= self.patience:
                if self.verbose:
                    print("Early stopping")
                self.early_stop = True
        else:
            self.best_score = score
            self.epochs_no_improve = 0

def validate_model(model, X_val, y_val, loss_fn):
    model.eval()
    with torch.no_grad():
        outputs = model(X_val)
        loss = loss_fn(outputs, y_val)
    return loss.item()

def train_model(model, X_train, y_train, X_val, y_val, loss_fn, optimizer, batch_size, epochs, early_stopping, scheduler):
    model.train()
    num_batches = len(X_train) // batch_size
    for epoch in range(epochs):
        epoch_loss = L_joint
        for i in range(num_batches):
            batch_x = X_train[i * batch_size:(i + 1) * batch_size]
            batch_y = y_train[i * batch_size:(i + 1) * batch_size]
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = loss_fn(outputs, batch_y)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            scheduler.step()
            epoch_loss += loss.item()
        val_loss = validate_model(model, X_val, y_val, loss_fn)
        # print(f"Epoch [{epoch+1}/{epochs}] Loss: {epoch_loss / num_batches:.4f}, Val Loss: {val_loss:.4f}")
        early_stopping(val_loss, model)
        if early_stopping.early_stop:
            break

early_stopping = EarlyStopping()
train_model(model, X_train, y_train, X_val, y_val, loss_fn, optimizer, batch_size, epochs, early_stopping, scheduler)

# Threshold optimization
def calculate_optimal_thresholds(y_true, y_scores):
    optimal_thresholds = []
    for i in range(y_true.shape[1]):
        precision, recall, thresholds = precision_recall_curve(y_true[:, i], y_scores[:, i])
        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)
        optimal_idx = np.nanargmax(f1_scores)
        best_threshold = thresholds[optimal_idx] if optimal_idx < len(thresholds) else 0.5
        optimal_thresholds.append(best_threshold)
    return optimal_thresholds

# Final evaluation
model.eval()
with torch.no_grad():
    y_scores_val = model(X_val).numpy()
    y_scores_test = model(X_test).numpy()

optimal_thresholds = calculate_optimal_thresholds(y_val.numpy(), y_scores_val)

y_pred_labels = np.zeros_like(y_scores_test)
for i, threshold in enumerate(optimal_thresholds):
    y_pred_labels[:, i] = (y_scores_test[:, i] > threshold).astype(int)
f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro')
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro')

print(f"MLP Results (%)")
print(f"F1-micro : {f1_micro_sklearn:.4f}")
print(f"F1-macro : {f1_macro_sklearn:.4f}")

"""BERT"""

R = 125
# Set random seed for reproducibility
np.random.seed(R)
tf.random.set_seed(R)

# Load and preprocess the data
data = pd.read_csv('/content/project/project/dataset/Fused-Arabic/dataset_Combined_Arabic.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

# Load the saved BERT embeddings
embeddings = joblib.load('/content/project/project/dataset/Fused-Arabic/bert_embeddings_Merge_arabic.pkl')
X = np.array(embeddings)

# List of personality traits (labels)
traits = ['E', 'N', 'A', 'C', 'O']

# Extract labels
labels = data[['E', 'N', 'A', 'C', 'O']]
y = labels.to_numpy()

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=12)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=12)

# Loss Function
class MultiTaskBCE(Loss):
    def __init__(self, num_tasks: int, task_weights=0.02) -> None:
        super().__init__()
        if task_weights is None:
            self.task_weights = tf.ones((1, num_tasks))
        elif tf.rank(task_weights) == 1:
            self.task_weights = tf.expand_dims(task_weights, 0)
        else:
            self.task_weights = task_weights

    def call(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:
        bce = binary_crossentropy(y_true, y_pred)
        loss = self.task_weights * tf.reduce_mean(bce, axis=0)
        loss = tf.reduce_sum(loss)
        return loss

# Transformer Block (replaces CNN)
class TransformerBlock(Model):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.01):
        super(TransformerBlock, self).__init__()
        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = Sequential([
            Dense(ff_dim, activation="relu"),
            Dense(embed_dim),
        ])
        self.layernorm1 = LayerNormalization(epsilon=1e-2)
        self.layernorm2 = LayerNormalization(epsilon=1e-2)
        self.dropout1 = Dropout(rate)
        self.dropout2 = Dropout(rate)

    def call(self, inputs, training=False):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

# Expert using Transformer instead of CNN
class ExpertTransformer(Model):
    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):
        super().__init__()
        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim, dropout_rate)
        self.pool = GlobalAveragePooling1D()
        self.dense = Dense(ff_dim, activation="relu")
        self.dropout = Dropout(dropout_rate)

    def call(self, inputs, training=False):
        x = tf.expand_dims(inputs, 1)  # Add sequence dimension
        x = self.transformer_block(x, training=training)
        x = self.pool(x)
        x = self.dense(x)
        x = self.dropout(x, training=training)
        return x

# MGMOE Model
class MultiGateMixtureOfExperts(Model):
    def __init__(self, num_tasks: int, embed_dim: int, num_experts: int = 3, num_heads: int = 4, ff_dim: int = 128, dropout_rate: float = 0.1):
        super().__init__()
        self.experts = [ExpertTransformer(embed_dim, num_heads, ff_dim, dropout_rate) for _ in range(num_experts)]
        self.gates = [Dense(num_experts, activation="softmax", use_bias=False) for _ in range(num_tasks)]
        self.towers = [Sequential([
            Dense(64, activation="relu"),
            Dropout(dropout_rate),
            Dense(1)
        ]) for _ in range(num_tasks)]

    def call(self, inputs, training=False):
        expert_outputs = [expert(inputs, training=training) for expert in self.experts]
        expert_outputs = tf.stack(expert_outputs, axis=1)  # (batch_size, num_experts, hidden_dim)
        task_outputs = []
        for gate, tower in zip(self.gates, self.towers):
            gate_scores = gate(inputs)  # (batch_size, num_experts)
            mixed_expert = tf.einsum('bnd,bn->bd', expert_outputs, gate_scores)  # Correct einsum
            output = tower(mixed_expert, training=training)
            task_outputs.append(output)
        out = tf.concat(task_outputs, axis=-1)
        out = tf.nn.sigmoid(out)
        return out

# Create and compile the model
num_tasks = 5
embed_dim = X_train.shape[1]

model = MultiGateMixtureOfExperts(
    num_tasks=num_tasks,
    embed_dim=embed_dim,
    num_experts=3,
    num_heads=8,
    ff_dim=64,
    dropout_rate=0.1
)

loss = MultiTaskBCE(num_tasks=num_tasks)
optimizer = tf.keras.optimizers.Adam()

model.compile(optimizer=optimizer, loss=loss, metrics=[Accuracy()])

# Train the model
history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32)

# Evaluate the model
y_pred = model.predict(X_test)
y_pred_labels = (y_pred > 0.5).astype(int)

lambda_f = accuracy_score(y_test, y_pred_labels)
# SHAP KernelExplainer Integration
def model_predict(X_np):
    X_tensor = tf.convert_to_tensor(X_np, dtype=tf.float32)
    y_pred = model(X_tensor, training=False).numpy()
    return y_pred

# Create SHAP explainer object for KernelExplainer
explainer = shap.KernelExplainer(model_predict, X_train[:4])  # Use a small subset of the training data for background
shap_values = explainer.shap_values(X_test[:4])  # Explain the first 10 samples from the test set

# Visualize SHAP values
# shap.summary_plot(shap_values, X_test[:4])  # Visualize SHAP values for the test set

# Compute Wasserstein distance loss
def wasserstein_loss(shap_values, reference_values):
    shap_values = np.array(shap_values)
    reference_values = np.array(reference_values)
    return np.mean([wasserstein_distance(shap_values[i].flatten(), reference_values[i].flatten())
                    for i in range(shap_values.shape[0])])

L_exp = wasserstein_loss(shap_values, np.mean(shap_values, axis=1))

# Compute the Joint Training Loss
L_joint = (1 - lambda_f) * history.history['loss'][-1] + lambda_f * L_exp

# Transformer-based Expert
class TransformerBlock(nn.Module):
    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):
        super().__init__()
        self.att = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)
        self.ffn = nn.Sequential(
            nn.Linear(embed_dim, ff_dim),
            nn.ReLU(),
            nn.Linear(ff_dim, embed_dim)
        )
        self.norm1 = nn.LayerNorm(embed_dim, eps=1e-2)
        self.norm2 = nn.LayerNorm(embed_dim, eps=1e-2)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

    def forward(self, x):
        attn_output, _ = self.att(x, x, x)
        x = self.norm1(x + self.dropout1(attn_output))
        ffn_output = self.ffn(x)
        return self.norm2(x + self.dropout2(ffn_output))

class TransformerExpert(nn.Module):
    def __init__(self, embed_dim, num_heads=4, ff_dim=128, dropout=0.1):
        super().__init__()
        self.transformer = TransformerBlock(embed_dim, num_heads, ff_dim, dropout)
        self.pool = nn.AdaptiveAvgPool1d(1)
        self.fc = nn.Linear(embed_dim, 64)
        self.out = nn.Linear(64, 1)
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = x.unsqueeze(1)  # (batch, 1, embed_dim)
        x = self.transformer(x)  # (batch, 1, embed_dim)
        x = x.transpose(1, 2)  # (batch, embed_dim, 1)
        x = self.pool(x).squeeze(-1)  # (batch, embed_dim)
        x = self.relu(self.fc(x))
        x = self.dropout(x)
        return torch.sigmoid(self.out(x)).squeeze()

# MGMOE with Transformer Experts
class MultiGateMixtureOfExperts(nn.Module):
    def __init__(self, input_dim, num_tasks, num_experts, ExpertNet):
        super().__init__()
        self.experts = nn.ModuleList([ExpertNet(input_dim) for _ in range(num_experts)])
        self.gates = nn.ParameterList([nn.Parameter(torch.randn(input_dim, num_experts)) for _ in range(num_tasks)])

    def forward(self, inputs):
        expert_outputs = torch.stack([expert(inputs) for expert in self.experts], dim=-1)
        task_outputs = []
        for gate in self.gates:
            gate_weights = torch.softmax(inputs @ gate, dim=-1)
            task_output = torch.sum(expert_outputs * gate_weights, dim=-1)
            task_outputs.append(task_output)
        return torch.stack(task_outputs, dim=-1)

# EarlyStopping
class EarlyStopping:
    def __init__(self, patience=10, verbose=True, delta=0.0001):
        self.patience = patience
        self.verbose = verbose
        self.delta = delta
        self.best_score = None
        self.epochs_no_improve = 0
        self.early_stop = False

    def __call__(self, val_loss, model):
        score = -val_loss
        if self.best_score is None or score > self.best_score + self.delta:
            self.best_score = score
            self.epochs_no_improve = 0
        else:
            self.epochs_no_improve += 1
            if self.epochs_no_improve >= self.patience:
                if self.verbose:
                    print("Early stopping")
                self.early_stop = True

# Validation & Training
def validate_model(model, X_val, y_val, loss_fn):
    model.eval()
    with torch.no_grad():
        outputs = model(X_val)
        loss = loss_fn(outputs, y_val)
    return loss.item()

def train_model(model, X_train, y_train, X_val, y_val, loss_fn, optimizer, batch_size, epochs, early_stopping, scheduler):
    model.train()
    num_batches = len(X_train) // batch_size
    for epoch in range(epochs):
        epoch_loss = L_joint   #L_joint
        for i in range(num_batches):
            batch_x = X_train[i * batch_size:(i + 1) * batch_size]
            batch_y = y_train[i * batch_size:(i + 1) * batch_size]
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = loss_fn(outputs, batch_y)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            scheduler.step()
            epoch_loss += loss.item()
        val_loss = validate_model(model, X_val, y_val, loss_fn)
        # print(f"Epoch [{epoch+1}/{epochs}] Loss: {epoch_loss / num_batches:.4f}, Val Loss: {val_loss:.4f}")
        early_stopping(val_loss, model)
        if early_stopping.early_stop:
            break
np.random.seed(R)
torch.manual_seed(R)
# Load and preprocess the data
data = pd.read_csv('/content/project/project/dataset/Fused-Arabic/dataset_Combined_Arabic.csv')
data.dropna(inplace=True)
labels = data[['E', 'N', 'A', 'C', 'O']]
embeddings = joblib.load('/content/project/project/dataset/Fused-Arabic/bert_embeddings_Merge_arabic.pkl')
X = np.array(embeddings)
y = np.clip(np.column_stack([labels[col] for col in labels.columns]), 0, 1).astype(np.float32)

X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=R)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.331, random_state=R)

X_train = torch.tensor(X_train, dtype=torch.float32)
X_val = torch.tensor(X_val, dtype=torch.float32)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32)
y_val = torch.tensor(y_val, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.float32)

# Run training and evaluation
input_dim = X_train.shape[1]
num_tasks = 5
num_experts = 3
batch_size = 64
epochs = 30

model = MultiGateMixtureOfExperts(input_dim, num_tasks, num_experts, TransformerExpert)
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
scheduler = OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(X_train) // batch_size, epochs=epochs)
loss_fn = nn.BCELoss()
early_stopping = EarlyStopping(patience=10, verbose=True)

train_model(model, X_train, y_train, X_val, y_val, loss_fn, optimizer, batch_size, epochs, early_stopping, scheduler)

# Threshold optimization
def calculate_optimal_thresholds(y_true, y_scores):
    optimal_thresholds = []
    for i in range(y_true.shape[1]):
        precision, recall, thresholds = precision_recall_curve(y_true[:, i], y_scores[:, i])
        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)
        optimal_idx = np.nanargmax(f1_scores)
        best_threshold = thresholds[optimal_idx] if optimal_idx < len(thresholds) else 0.5
        optimal_thresholds.append(best_threshold)
        # print(f"Class {i}: Best Threshold = {best_threshold:.4f}, Max F1 = {f1_scores[optimal_idx]:.4f}")
    return optimal_thresholds

# Final evaluation
def evaluate_model_with_optimal_thresholds(model, X_test, y_test, optimal_thresholds):
    model.eval()
    with torch.no_grad():
        y_scores = model(X_test).numpy()
        y_pred_labels = np.zeros_like(y_scores)
        for i, threshold in enumerate(optimal_thresholds):
            y_pred_labels[:, i] = (y_scores[:, i] > threshold).astype(int)
        # acc = accuracy_score(y_test.numpy(), y_pred_labels) * 100
        f1_micro = f1_score(y_test.numpy(), y_pred_labels, average='micro')
        f1_macro = f1_score(y_test.numpy(), y_pred_labels, average='macro')
        print(f"BERT Results (%)")
        print(f"F1-micro: {f1_micro:.4f}")
        print(f"F1-macro: {f1_macro:.4f}")

# Final Evaluation
y_scores_val = model(X_val).detach().numpy()
optimal_thresholds = calculate_optimal_thresholds(y_val.numpy(), y_scores_val)
evaluate_model_with_optimal_thresholds(model, X_test, y_test, optimal_thresholds)

"""LR"""

# Set random seed for reproducibility
R = 26
np.random.seed(R)
torch.manual_seed(R)

# Load and preprocess the data
data = pd.read_csv('/content/project/project/dataset/Fused-Arabic/dataset_Combined_Arabic.csv')
data.dropna(inplace=True)
# Extract text data and labels
texts = data['text']
labels = data[['E', 'N', 'A', 'C', 'O']]

# Load the saved BERT embeddings
embeddings = joblib.load('/content/project/project/dataset/Fused-Arabic/bert_embeddings_Merge_arabic.pkl')
X = np.array(embeddings)

# Define labels for each task
y = np.column_stack([labels[col] for col in ['E', 'N', 'A', 'C', 'O']])

# Ensure the labels are binary (0 or 1) and convert to float tensors
y = np.clip(y, 0, 1)
y = y.astype(np.float32)

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=R1)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=R1)

# Convert data to PyTorch tensors
X_train = torch.tensor(X_train, dtype=torch.float32)
X_val = torch.tensor(X_val, dtype=torch.float32)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32)
y_val = torch.tensor(y_val, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.float32)

# Define PyTorch Logistic Regression Model
class LogisticRegression(nn.Module):
    def __init__(self, input_dim):
        super(LogisticRegression, self).__init__()
        self.linear = nn.Linear(input_dim, 1)

    def forward(self, x):
        logits = self.linear(x)
        return torch.sigmoid(logits).squeeze()


# MGMOE Model with Logistic Regression Experts using PyTorch
class MultiGateMixtureOfExperts(nn.Module):
    def __init__(self, input_dim, num_tasks, num_experts, LogisticRegression):
        super(MultiGateMixtureOfExperts, self).__init__()
        self.num_tasks = num_tasks
        self.num_experts = num_experts
        self.experts = nn.ModuleList([LogisticRegression(input_dim) for _ in range(num_experts)])
        self.gates = nn.ParameterList([nn.Parameter(torch.randn(input_dim, num_experts)) for _ in range(num_tasks)])

    def forward(self, inputs):
        out_experts = torch.stack([expert(inputs) for expert in self.experts], dim=-1)
        out_tasks = []
        for gate in self.gates:
            gate_scores = torch.softmax(inputs @ gate, dim=-1)
            task_output = torch.sum(out_experts * gate_scores, dim=-1)
            out_tasks.append(task_output)
        out = torch.stack(out_tasks, dim=-1)
        return torch.sigmoid(out)

# Define Mean Squared Error (MSE) Loss Function
def loss(y_true, y_pred):
    criterion = torch.nn.MSELoss()
    return criterion(y_pred, y_true)

# Hyperparameters
input_dim = X_train.shape[1]
num_tasks = 5
num_experts = 3
learning_rate = 0.001
epochs = 20
batch_size = 32

# Instantiate the model and optimizer
model = MultiGateMixtureOfExperts(input_dim=input_dim, num_tasks=num_tasks, num_experts=num_experts, LogisticRegression=LogisticRegression)
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
def train_model(model, X_train, y_train, optimizer, batch_size, epochs):
    global L_label
    model.train()
    num_batches = len(X_train) // batch_size
    for epoch in range(epochs):
        epoch_loss = 0
        for i in range(num_batches):
            batch_x = X_train[i*batch_size:(i+1)*batch_size]
            batch_y = y_train[i*batch_size:(i+1)*batch_size]
            outputs = model(batch_x)
            loss_value = loss(batch_y, outputs)
            epoch_loss += loss_value.item()
            optimizer.zero_grad()
            loss_value.backward()
            optimizer.step()
        L_label = epoch_loss / num_batches  # Store last epoch loss
        # print(f'Epoch [{epoch+1}/{epochs}], Loss: {L_label:.4f}')

# Train the model
train_model(model, X_train, y_train, optimizer, batch_size, epochs)

# Evaluate the model
model.eval()
with torch.no_grad():
    y_pred = model(X_test).numpy()
    y_pred_labels = (y_pred > 0.5).astype(int)

# Calculate accuracy for λ_f
lambda_f = accuracy_score(y_test.numpy(), y_pred_labels)
# SHAP KernelExplainer Integration

# Define a prediction function for SHAP
def model_predict(X_np):
    X_tensor = torch.tensor(X_np, dtype=torch.float32)
    with torch.no_grad():
        return model(X_tensor).numpy()

# Select a small subset of the training data for SHAP KernelExplainer reference
X_sample = X_train[:50].numpy()
X_explain = X_test[:10].numpy()

# Apply SHAP KernelExplainer
explainer = shap.KernelExplainer(model_predict, X_sample)
shap_values = explainer.shap_values(X_explain)
# shap.summary_plot(shap_values, X_explain)

# Compute Wasserstein distance loss
def wasserstein_loss(shap_values, reference_values):
    shap_values = np.array(shap_values)
    reference_values = np.array(reference_values)
    return np.mean([wasserstein_distance(shap_values[i].flatten(), reference_values[i].flatten())
                    for i in range(shap_values.shape[0])])

L_exp = wasserstein_loss(shap_values, np.mean(shap_values, axis=1))

# Compute the Joint Training Loss
L_joint = (1 - lambda_f) * L_label + lambda_f * L_exp

# Define Focal Loss for handling class imbalance
class FocalLoss(nn.Module):
    def __init__(self, gamma=2.0, alpha=0.25):
        super(FocalLoss, self).__init__()
        self.gamma = gamma
        self.alpha = alpha  # Adjust alpha based on class distribution if known

    def forward(self, inputs, targets):
        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        targets = targets.type(torch.float32)
        at = self.alpha * targets + (1 - self.alpha) * (1 - targets)
        pt = torch.exp(-BCE_loss)
        F_loss = at * (1 - pt) ** self.gamma * BCE_loss
        return F_loss.mean()

# Define the model with additional layers and modified activation
class LogisticRegression1(nn.Module):
    def __init__(self, input_dim):
        super(LogisticRegression1, self).__init__()
        self.linear1 = nn.Linear(input_dim, input_dim // 2)
        self.relu = nn.ReLU()  # Using ReLU activation
        self.linear2 = nn.Linear(input_dim // 2, 1)
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        x = self.dropout(self.relu(self.linear1(x)))
        x = torch.sigmoid(self.linear2(x))
        return x.squeeze()
# Set random seed for reproducibility
np.random.seed(R)
torch.manual_seed(R)

# Load and preprocess the data
data = pd.read_csv('/content/project/project/dataset/Fused-Arabic/dataset_Combined_Arabic.csv')
data.dropna(inplace=True)
texts = data['text']
labels = data[['E', 'N', 'A', 'C', 'O']]

# Load the saved BERT embeddings
embeddings = joblib.load('/content/project/project/dataset/Fused-Arabic/bert_embeddings_Merge_arabic.pkl')
X = np.array(embeddings)

# Define labels for each task
y = np.column_stack([labels[col] for col in ['E', 'N', 'A', 'C', 'O']])

# Ensure the labels are binary (0 or 1) and convert to float tensors
y = np.clip(y, 0, 1)
y = y.astype(np.float32)

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=R)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=R)

# Convert data to PyTorch tensors
X_train = torch.tensor(X_train, dtype=torch.float32)
X_val = torch.tensor(X_val, dtype=torch.float32)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32)
y_val = torch.tensor(y_val, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.float32)

# Instantiate the model, loss function, and optimizer
input_dim = X_train.shape[1]
num_tasks = 5
num_experts = 3
learning_rate = 0.001
batch_size = 256
epochs = 30

# Adjust the optimizer setup and include OneCycleLR
model = MultiGateMixtureOfExperts(input_dim=input_dim, num_tasks=num_tasks, num_experts=num_experts, LogisticRegression=LogisticRegression1)
# model = MultiGateMixtureOfExperts(input_dim, num_tasks, num_experts, LogisticRegression1)
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # Adding L2 regularization
scheduler = OneCycleLR(optimizer, max_lr=0.5, steps_per_epoch=len(X_train) // 32, epochs=30)
focal_loss = FocalLoss()

# Early Stopping and Learning Rate Scheduler
class EarlyStopping:
    def __init__(self, patience=10, verbose=True, delta=0.0001):
        self.patience = patience
        self.verbose = verbose
        self.delta = delta
        self.best_score = None
        self.epochs_no_improve = 0
        self.early_stop = False

    def __call__(self, val_loss, model):
        score = -val_loss
        if self.best_score is None:
            self.best_score = score
        elif score < self.best_score + self.delta:
            self.epochs_no_improve += 1
            if self.epochs_no_improve >= self.patience:
                if self.verbose:
                    print("Early stopping")
                self.early_stop = True
        else:
            self.best_score = score
            self.epochs_no_improve = 0

# Update the training loop to integrate OneCycleLR
def train_model(model, X_train, y_train, X_val, y_val, loss_fn, optimizer, batch_size, epochs, early_stopping, scheduler, L_joint):
    model.train()
    num_batches = len(X_train) // batch_size
    for epoch in range(epochs):
        epoch_loss = L_joint
        for i in range(num_batches):
            batch_x = X_train[i * batch_size:(i + 1) * batch_size]
            batch_y = y_train[i * batch_size:(i + 1) * batch_size]
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = loss_fn(outputs, batch_y)
            loss.backward()
            optimizer.step()
            scheduler.step()  # Update the learning rate after each batch
            epoch_loss += loss.item()
        val_loss = validate_model(model, X_val, y_val, loss_fn)
        # print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss / num_batches:.4f}, Val Loss: {val_loss:.4f}, LR: {scheduler.get_last_lr()[0]}')
        early_stopping(val_loss, model)
        if early_stopping.early_stop:
            print("........................")
            break


# Validation function
def validate_model(model, X_val, y_val, loss_fn):
    model.eval()
    with torch.no_grad():
        outputs = model(X_val)
        loss = loss_fn(outputs, y_val)
    return loss.item()

early_stopping = EarlyStopping(patience=10, verbose=True)

# Execute training
train_model(model, X_train, y_train, X_val, y_val, focal_loss, optimizer, batch_size, epochs, early_stopping, scheduler, L_joint)

# Function to calculate the optimal thresholds based on precision-recall curves
def calculate_optimal_thresholds(y_true, y_scores):
    optimal_thresholds = []
    for i in range(y_true.shape[1]):
        precision, recall, thresholds = precision_recall_curve(y_true[:, i], y_scores[:, i])
        f1_scores = 2 * (precision * recall) / (precision + recall+0.0000001)
        optimal_idx = np.nanargmax(f1_scores)
        optimal_thresholds.append(thresholds[optimal_idx] if optimal_idx < len(thresholds) else 0.5)
    return optimal_thresholds

# Evaluate the model using the optimal thresholds
y_scores_val = model(X_val).detach().numpy()
optimal_thresholds = calculate_optimal_thresholds(y_val.numpy(), y_scores_val)

def evaluate_model_with_optimal_thresholds(model, X_test, y_test, optimal_thresholds):
    model.eval()
    with torch.no_grad():
        y_scores = model(X_test).numpy()
        y_pred_labels = np.zeros_like(y_scores)
        for i, threshold in enumerate(optimal_thresholds):
            y_pred_labels[:, i] = (y_scores[:, i] > threshold).astype(int)
        # accuracy = accuracy_score(y_test.numpy(), y_pred_labels) * 100
        f1_micro = f1_score(y_test.numpy(), y_pred_labels, average='micro')
        f1_macro = f1_score(y_test.numpy(), y_pred_labels, average='macro')
    print(f"LR Results (%)")
    print(f"F1-micro: {f1_micro:.4f}")
    print(f"F1-macro: {f1_macro:.4f}")

# Run final evaluation
evaluate_model_with_optimal_thresholds(model, X_test, y_test, optimal_thresholds)

"""XGB"""

R = 17
# Set seeds
np.random.seed(R)
torch.manual_seed(R)

# Load and preprocess data
data = pd.read_csv('/content/project/project/dataset/Fused-Arabic/dataset_Combined_Arabic.csv')
data.dropna(inplace=True)
labels = data[['E', 'N', 'A', 'C', 'O']]
embeddings = joblib.load('/content/project/project/dataset/Fused-Arabic/bert_embeddings_Merge_arabic.pkl')
X = np.array(embeddings)
y = labels.to_numpy()
y = np.where(y == -1, 0, y)

# Split
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=R1)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=R1)

# Convert tensors for gating
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32)
y_val_tensor = torch.tensor(y_val, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32)

# Define Expert class using XGBoost
class XGBExpert:
    def __init__(self):
        self.model = XGBClassifier(
            n_estimators=100,
            max_depth=2,
            learning_rate=0.001,
            objective='binary:logistic',
            eval_metric='mlogloss',
            # use_label_encoder=False,
            random_state=42
        )

    def train(self, X, y):
        self.model.fit(X, y)

    def predict_proba(self, X):
        return self.model.predict_proba(X)[:, 1]  # Only positive class

# MGMOE model using PyTorch gates and XGB experts
class MultiGateMixtureOfExperts:
    def __init__(self, input_dim, num_tasks, num_experts):
        self.num_tasks = num_tasks
        self.num_experts = num_experts
        self.experts = [XGBExpert() for _ in range(num_experts)]
        self.gates = torch.nn.ParameterList([
            torch.nn.Parameter(torch.randn(input_dim, num_experts, requires_grad=True))
            for _ in range(num_tasks)
        ])
        self.optimizer = torch.optim.Adam(self.gates.parameters(), lr=0.01)

    def train_experts(self, X, y):
        for i in range(self.num_experts):
            self.experts[i].train(X, y[:, i])

    def forward(self, X):
        if isinstance(X, np.ndarray):
            X = torch.tensor(X, dtype=torch.float32)

        with torch.no_grad():
            expert_outputs = [torch.tensor(expert.predict_proba(X.numpy()), dtype=torch.float32)
                              for expert in self.experts]
            expert_outputs = torch.stack(expert_outputs, dim=-1)  # (batch, experts)

        outputs = []
        for i, gate in enumerate(self.gates):
            gate_scores = F.softmax(torch.matmul(X, gate), dim=-1)  # (batch, experts)
            task_output = torch.sum(expert_outputs * gate_scores, dim=-1)  # (batch,)
            outputs.append(task_output)

        return torch.stack(outputs, dim=-1)  # (batch, tasks)

    def train_gates(self, X_tensor, y_tensor, epochs=30, batch_size=64):
        global L_label
        for epoch in range(epochs):
            epoch_loss = 0
            for i in range(0, len(X_tensor), batch_size):
                batch_x = X_tensor[i:i + batch_size]
                batch_y = y_tensor[i:i + batch_size]
                preds = self.forward(batch_x)
                loss_val = F.mse_loss(preds, batch_y)
                self.optimizer.zero_grad()
                loss_val.backward()
                self.optimizer.step()
                epoch_loss += loss_val.item()
            L_label = epoch_loss / (len(X_tensor) // batch_size)
            # print(f"Epoch {epoch + 1}/{epochs}, Loss: {L_label:.4f}")
# Hyperparameters
input_dim = X_train.shape[1]
num_tasks = 5
num_experts = 3
# Initialize model
model = MultiGateMixtureOfExperts(input_dim, num_tasks, num_experts)

# Train experts (on each task)
model.train_experts(X_train, y_train)

# Train gates using PyTorch
model.train_gates(X_train_tensor, y_train_tensor, epochs=30, batch_size=64)

# Evaluate
model_output = model.forward(X_test_tensor)
y_pred = model_output.detach().numpy()
y_pred_labels = (y_pred > 0.5).astype(int)

# Metrics
lambda_f = accuracy_score(y_test, y_pred_labels)
# ----------------------------------
# SHAP Integration
# ----------------------------------
def model_predict(X_np):
    return model.forward(X_np).detach().numpy()

X_sample = X_train[:50]
X_explain = X_test[:10]
explainer = shap.KernelExplainer(model_predict, X_sample)
shap_values = explainer.shap_values(X_explain)
# shap.summary_plot(shap_values, X_explain)

# Wasserstein Distance Loss
def wasserstein_loss(shap_values, reference_values):
    shap_values = np.array(shap_values)
    reference_values = np.array(reference_values)
    return np.mean([
        wasserstein_distance(shap_values[i].flatten(), reference_values[i].flatten())
        for i in range(shap_values.shape[0])
    ])

L_exp = wasserstein_loss(shap_values, np.mean(shap_values, axis=1))
L_joint = (1 - lambda_f) * L_label + lambda_f * L_exp

# ===========================
# XGBClassifier Expert Class
# ===========================
class XGBExpert:
    def __init__(self):
        # Create an XGBClassifier with optimized hyperparameters
        self.model = XGBClassifier(
            n_estimators=50,
            max_depth=2,
            learning_rate=0.1,
            objective='binary:logistic',
            eval_metric='mlogloss',
            random_state=42
        )

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict_proba(self, X):
        # XGBClassifier provides probability estimates
        return self.model.predict_proba(X)[:, 1]  # Return the probability of the positive class
# Set random seeds
np.random.seed(R)
torch.manual_seed(R)

# Load and preprocess data
data = pd.read_csv('/content/project/project/dataset/Fused-Arabic/dataset_Combined_Arabic.csv')
data.dropna(inplace=True)
texts = data['text']
labels = data[['E', 'N', 'A', 'C', 'O']]

# Load BERT embeddings
embeddings = joblib.load('/content/project/project/dataset/Fused-Arabic/bert_embeddings_Merge_arabic.pkl')
X = np.array(embeddings)
y = np.column_stack([labels[col] for col in ['E', 'N', 'A', 'C', 'O']])
y = np.clip(y, 0, 1).astype(np.float32)

# Train/val/test split
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=R)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=R)

# Convert val/test to torch tensors
X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_val_tensor = torch.tensor(y_val, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32)
# Train multiple XGB experts
num_experts = 3
xgb_experts = [[XGBExpert() for _ in range(num_experts)] for _ in range(5)]  # 5 tasks × N experts

# Train each expert on full training data
for task_idx in range(5):
    for expert_idx in range(num_experts):
        xgb_experts[task_idx][expert_idx].train(X_train, y_train[:, task_idx])

# =============================
# Gating Network (PyTorch)
# =============================
class MGMOE_GatingOnly(nn.Module):
    def __init__(self, input_dim, num_tasks, num_experts):
        super(MGMOE_GatingOnly, self).__init__()
        self.num_tasks = num_tasks
        self.num_experts = num_experts
        self.gates = nn.ParameterList([
            nn.Parameter(torch.randn(input_dim, num_experts)) for _ in range(num_tasks)
        ])

    def forward(self, x, expert_outputs):  # expert_outputs: shape (batch, tasks, experts)
        batch_size = x.shape[0]
        task_outputs = []
        for t in range(self.num_tasks):
            gate_logits = x @ self.gates[t]  # (batch, experts)
            gate_weights = torch.softmax(gate_logits, dim=-1)
            weighted_output = (expert_outputs[:, t, :] * gate_weights).sum(dim=-1)
            task_outputs.append(weighted_output)
        return torch.stack(task_outputs, dim=-1)  # (batch, tasks)

# ===============================
# Focal Loss for Multilabel Task
# ===============================
class FocalLoss(nn.Module):
    def __init__(self, gamma=2.0, alpha=0.25):
        super(FocalLoss, self).__init__()
        self.gamma = gamma
        self.alpha = alpha

    def forward(self, inputs, targets):
        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        at = self.alpha * targets + (1 - self.alpha) * (1 - targets)
        pt = torch.exp(-BCE_loss)
        loss = at * (1 - pt) ** self.gamma * BCE_loss
        return loss.mean()

# ========================================
# Prepare Expert Outputs for Gating Net
# ========================================
def get_expert_outputs(X):
    outputs = np.zeros((X.shape[0], 5, num_experts))
    for task_idx in range(5):
        for expert_idx in range(num_experts):
            prob = xgb_experts[task_idx][expert_idx].predict_proba(X)
            outputs[:, task_idx, expert_idx] = prob
    return torch.tensor(outputs, dtype=torch.float32)

# Get expert outputs for train/val
expert_outputs_train = get_expert_outputs(X_train)
expert_outputs_val = get_expert_outputs(X_val)

# Convert X_train to tensor
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32)

# ==========================================
# Training Gating Network (Only This Trains)
# ==========================================
input_dim = X.shape[1]
model = MGMOE_GatingOnly(input_dim, num_tasks=5, num_experts=num_experts)
optimizer = optim.Adam(model.parameters(), lr=0.01)
scheduler = OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(X_train) // 32, epochs=30)
loss_fn = FocalLoss()

# EarlyStopping class
class EarlyStopping:
    def __init__(self, patience=5, verbose=True, delta=0.0001):
        self.patience = patience
        self.verbose = verbose
        self.delta = delta
        self.best_score = None
        self.epochs_no_improve = 0
        self.early_stop = False

    def __call__(self, val_loss, model):
        score = -val_loss
        if self.best_score is None:
            self.best_score = score
        elif score < self.best_score + self.delta:
            self.epochs_no_improve += 1
            if self.epochs_no_improve >= self.patience:
                if self.verbose:
                    print("Early stopping triggered.")
                self.early_stop = True
        else:
            self.best_score = score
            self.epochs_no_improve = 0

# Training loop
def train_model(model, X_train, expert_outputs_train, y_train, X_val, expert_outputs_val, y_val, loss_fn, optimizer, scheduler, epochs=30, batch_size=32):
    early_stopping = EarlyStopping()
    for epoch in range(epochs):
        model.train()
        permutation = torch.randperm(X_train.size(0))
        epoch_loss = L_joint
        for i in range(0, X_train.size(0), batch_size):
            indices = permutation[i:i + batch_size]
            batch_x = X_train[indices]
            batch_y = y_train[indices]
            batch_exp = expert_outputs_train[indices]

            optimizer.zero_grad()
            outputs = model(batch_x, batch_exp)
            loss = loss_fn(outputs, batch_y)
            loss.backward()
            optimizer.step()
            scheduler.step()
            epoch_loss += loss.item()

        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val, expert_outputs_val)
            val_loss = loss_fn(val_outputs, y_val)
        # print(f"Epoch {epoch+1}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}")
        early_stopping(val_loss.item(), model)
        if early_stopping.early_stop:
            break

train_model(model, X_train_tensor, expert_outputs_train, y_train_tensor,
            X_val_tensor, expert_outputs_val, y_val_tensor,
            loss_fn, optimizer, scheduler)

# ==========================================
# Evaluation with Optimal Thresholds
# ==========================================
def calculate_optimal_thresholds(y_true, y_scores):
    thresholds = []
    for i in range(y_true.shape[1]):
        precision, recall, thresh = precision_recall_curve(y_true[:, i], y_scores[:, i])
        f1 = 2 * (precision * recall) / (precision + recall + 1e-8)
        best_thresh = thresh[np.argmax(f1)] if len(thresh) > 0 else 0.5
        thresholds.append(best_thresh)
    return thresholds

# Evaluate on test
expert_outputs_test = get_expert_outputs(X_test)
optimal_thresholds = calculate_optimal_thresholds(y_val, model(X_val_tensor, expert_outputs_val).detach().numpy())
model.eval()
with torch.no_grad():
    y_scores_test = model(X_test_tensor, expert_outputs_test).detach().numpy()
    preds_bin = np.zeros_like(y_scores_test)

    for i in range(5):
        preds_bin[:, i] = (y_scores_test[:, i] > optimal_thresholds[i]).astype(int)
    f1_micro = f1_score(y_test, preds_bin, average='micro')
    f1_macro = f1_score(y_test, preds_bin, average='macro')

print(f"XGB Results (%)")
print(f"F1 Micro: {f1_micro:.4f}")
print(f"F1 Macro: {f1_macro:.4f}")

"""RF"""

R = 26
# Set seeds
np.random.seed(R)
torch.manual_seed(R)

# Load and preprocess data
data = pd.read_csv('/content/project/project/dataset/Fused-Arabic/dataset_Combined_Arabic.csv')
data.dropna(inplace=True)
traits = ['E', 'N', 'A', 'C', 'O']
labels = data[['E', 'N', 'A', 'C', 'O']]
embeddings = joblib.load('/content/project/project/dataset/Fused-Arabic/bert_embeddings_Merge_arabic.pkl')
X = np.array(embeddings)
y = labels.to_numpy()
y = np.where(y == -1, 0, y)

# Split
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=R1)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=R1)

# Convert tensors for gating
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32)
y_val_tensor = torch.tensor(y_val, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32)

# Define Expert class using RandomForestClassifier
class RandomForestExpert:
    def __init__(self, num_trees=500, max_depth=5, min_samples_split=2, min_samples_leaf=2,
                 min_weight_fraction_leaf=0.001, max_features='sqrt', ccp_alpha=0.01):
        self.model = RandomForestClassifier(
            n_estimators=num_trees,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            random_state=42,
            n_jobs=-1,
            ccp_alpha=ccp_alpha
        )

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict_proba(self, X):
        return self.model.predict_proba(X)[:, 1]

# MGMOE model using PyTorch gates and RandomForest experts
class MultiGateMixtureOfExperts:
    def __init__(self, input_dim, num_tasks, num_experts):
        self.num_tasks = num_tasks
        self.num_experts = num_experts
        self.experts = [RandomForestExpert() for _ in range(num_experts)]
        self.gates = torch.nn.ParameterList([
            torch.nn.Parameter(torch.randn(input_dim, num_experts, requires_grad=True))
            for _ in range(num_tasks)
        ])
        self.optimizer = torch.optim.Adam(self.gates.parameters(), lr=0.01)

    def train_experts(self, X, y):
        for i in range(self.num_experts):
            self.experts[i].train(X, y[:, i])

    def forward(self, X):
        if isinstance(X, np.ndarray):
            X = torch.tensor(X, dtype=torch.float32)

        with torch.no_grad():
            expert_outputs = [torch.tensor(expert.predict_proba(X.numpy()), dtype=torch.float32)
                              for expert in self.experts]
            expert_outputs = torch.stack(expert_outputs, dim=-1)  # (batch, experts)

        outputs = []
        for i, gate in enumerate(self.gates):
            gate_scores = F.softmax(torch.matmul(X, gate), dim=-1)  # (batch, experts)
            task_output = torch.sum(expert_outputs * gate_scores, dim=-1)  # (batch,)
            outputs.append(task_output)

        return torch.stack(outputs, dim=-1)  # (batch, tasks)

    def train_gates(self, X_tensor, y_tensor, epochs=30, batch_size=64):
        global L_label
        for epoch in range(epochs):
            epoch_loss = 0
            for i in range(0, len(X_tensor), batch_size):
                batch_x = X_tensor[i:i + batch_size]
                batch_y = y_tensor[i:i + batch_size]
                preds = self.forward(batch_x)
                loss_val = F.mse_loss(preds, batch_y)
                self.optimizer.zero_grad()
                loss_val.backward()
                self.optimizer.step()
                epoch_loss += loss_val.item()
            L_label = epoch_loss / (len(X_tensor) // batch_size)
            # print(f"Epoch {epoch + 1}/{epochs}, Loss: {L_label:.4f}")

# Hyperparameters
input_dim = X_train.shape[1]
num_tasks = 5
num_experts = 3

# Initialize model
model = MultiGateMixtureOfExperts(input_dim, num_tasks, num_experts)

# Train experts (on each task)
model.train_experts(X_train, y_train)

# Train gates using PyTorch
model.train_gates(X_train_tensor, y_train_tensor, epochs=30, batch_size=64)

# Evaluate
model_output = model.forward(X_test_tensor)
y_pred = model_output.detach().numpy()
y_pred_labels = (y_pred > 0.5).astype(int)

# Metrics
lambda_f = accuracy_score(y_test, y_pred_labels)
# ----------------------------------
# SHAP Integration
# ----------------------------------
def model_predict(X_np):
    return model.forward(X_np).detach().numpy()

X_sample = X_train[:50]
X_explain = X_test[:10]
explainer = shap.KernelExplainer(model_predict, X_sample)
shap_values = explainer.shap_values(X_explain)
# shap.summary_plot(shap_values, X_explain)

# Wasserstein Distance Loss
def wasserstein_loss(shap_values, reference_values):
    shap_values = np.array(shap_values)
    reference_values = np.array(reference_values)
    return np.mean([
        wasserstein_distance(shap_values[i].flatten(), reference_values[i].flatten())
        for i in range(shap_values.shape[0])
    ])

L_exp = wasserstein_loss(shap_values, np.mean(shap_values, axis=1))
L_joint = (1 - lambda_f) * L_label + lambda_f * L_exp

# ===========================
# RandomForest Expert Class
# ===========================
class RandomForestExpert:
    def __init__(self, num_trees=500, max_depth=4, min_samples_split=2, min_samples_leaf=3,
                 min_weight_fraction_leaf=0.001, max_features='sqrt', ccp_alpha=0.01):
        self.model = RandomForestClassifier(
            n_estimators=num_trees,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            random_state=42,
            n_jobs=-1,
            ccp_alpha=ccp_alpha
        )

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict_proba(self, X):
        return self.model.predict_proba(X)[:, 1]
# Set random seeds
np.random.seed(R)
torch.manual_seed(R)

# Load and preprocess data
data = pd.read_csv('/content/project/project/dataset/Fused-Arabic/dataset_Combined_Arabic.csv')
data.dropna(inplace=True)
texts = data['text']
labels = data[['E', 'N', 'A', 'C', 'O']]

# Load BERT embeddings
embeddings = joblib.load('/content/project/project/dataset/Fused-Arabic/bert_embeddings_Merge_arabic.pkl')
X = np.array(embeddings)
y = np.column_stack([labels[col] for col in ['E', 'N', 'A', 'C', 'O']])
y = np.clip(y, 0, 1).astype(np.float32)

# Train/val/test split
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=R)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=R)

# Convert val/test to torch tensors
X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_val_tensor = torch.tensor(y_val, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32)



# Train multiple Random Forest experts
num_experts = 3
rf_experts = [[RandomForestExpert() for _ in range(num_experts)] for _ in range(5)]  # 5 tasks × N experts

# Train each expert on full training data
for task_idx in range(5):
    for expert_idx in range(num_experts):
        rf_experts[task_idx][expert_idx].train(X_train, y_train[:, task_idx])

# =============================
# Gating Network (PyTorch)
# =============================
class MGMOE_GatingOnly(nn.Module):
    def __init__(self, input_dim, num_tasks, num_experts):
        super(MGMOE_GatingOnly, self).__init__()
        self.num_tasks = num_tasks
        self.num_experts = num_experts
        self.gates = nn.ParameterList([
            nn.Parameter(torch.randn(input_dim, num_experts)) for _ in range(num_tasks)
        ])

    def forward(self, x, expert_outputs):
        batch_size = x.shape[0]
        task_outputs = []
        for t in range(self.num_tasks):
            gate_logits = x @ self.gates[t]  # (batch, experts)
            gate_weights = torch.softmax(gate_logits, dim=-1)
            weighted_output = (expert_outputs[:, t, :] * gate_weights).sum(dim=-1)
            task_outputs.append(weighted_output)
        return torch.stack(task_outputs, dim=-1)  # (batch, tasks)

# ===============================
# Focal Loss for Multilabel Task
# ===============================
class FocalLoss(nn.Module):
    def __init__(self, gamma=2.0, alpha=0.25):
        super(FocalLoss, self).__init__()
        self.gamma = gamma
        self.alpha = alpha

    def forward(self, inputs, targets):
        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        at = self.alpha * targets + (1 - self.alpha) * (1 - targets)
        pt = torch.exp(-BCE_loss)
        loss = at * (1 - pt) ** self.gamma * BCE_loss
        return loss.mean()

# ========================================
# Prepare Expert Outputs for Gating Net
# ========================================
def get_expert_outputs(X):
    outputs = np.zeros((X.shape[0], 5, num_experts))
    for task_idx in range(5):
        for expert_idx in range(num_experts):
            prob = rf_experts[task_idx][expert_idx].predict_proba(X)
            outputs[:, task_idx, expert_idx] = prob
    return torch.tensor(outputs, dtype=torch.float32)

# Get expert outputs for train/val
expert_outputs_train = get_expert_outputs(X_train)
expert_outputs_val = get_expert_outputs(X_val)

# Convert X_train to tensor
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32)

# ==========================================
# Training Gating Network
# ==========================================
input_dim = X.shape[1]
model = MGMOE_GatingOnly(input_dim, num_tasks=5, num_experts=num_experts)
optimizer = optim.Adam(model.parameters(), lr=0.01)
scheduler = OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(X_train) // 32, epochs=30)
loss_fn = FocalLoss()

# EarlyStopping class
class EarlyStopping:
    def __init__(self, patience=5, verbose=True, delta=0.0001):
        self.patience = patience
        self.verbose = verbose
        self.delta = delta
        self.best_score = None
        self.epochs_no_improve = 0
        self.early_stop = False

    def __call__(self, val_loss, model):
        score = -val_loss
        if self.best_score is None:
            self.best_score = score
        elif score < self.best_score + self.delta:
            self.epochs_no_improve += 1
            if self.epochs_no_improve >= self.patience:
                if self.verbose:
                    print("Early stopping triggered.")
                self.early_stop = True
        else:
            self.best_score = score
            self.epochs_no_improve = 0

# Training loop
def train_model(model, X_train, expert_outputs_train, y_train, X_val, expert_outputs_val, y_val, loss_fn, optimizer, scheduler, epochs=30, batch_size=32):
    early_stopping = EarlyStopping()
    for epoch in range(epochs):
        model.train()
        permutation = torch.randperm(X_train.size(0))
        epoch_loss = L_joint
        for i in range(0, X_train.size(0), batch_size):
            indices = permutation[i:i + batch_size]
            batch_x = X_train[indices]
            batch_y = y_train[indices]
            batch_exp = expert_outputs_train[indices]

            optimizer.zero_grad()
            outputs = model(batch_x, batch_exp)
            loss = loss_fn(outputs, batch_y)
            loss.backward()
            optimizer.step()
            scheduler.step()
            epoch_loss += loss.item()

        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val, expert_outputs_val)
            val_loss = loss_fn(val_outputs, y_val)
        # print(f"Epoch {epoch+1}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}")
        early_stopping(val_loss.item(), model)
        if early_stopping.early_stop:
            break

train_model(model, X_train_tensor, expert_outputs_train, y_train_tensor,
            X_val_tensor, expert_outputs_val, y_val_tensor,
            loss_fn, optimizer, scheduler)

# ==========================================
# Evaluation with Optimal Thresholds
# ==========================================
def calculate_optimal_thresholds(y_true, y_scores):
    thresholds = []
    for i in range(y_true.shape[1]):
        precision, recall, thresh = precision_recall_curve(y_true[:, i], y_scores[:, i])
        f1 = 2 * (precision * recall) / (precision + recall + 1e-8)
        best_thresh = thresh[np.argmax(f1)] if len(thresh) > 0 else 0.5
        thresholds.append(best_thresh)
    return thresholds

# Evaluate on test
expert_outputs_test = get_expert_outputs(X_test)
optimal_thresholds = calculate_optimal_thresholds(y_val, model(X_val_tensor, expert_outputs_val).detach().numpy())
model.eval()
with torch.no_grad():
    y_scores_test = model(X_test_tensor, expert_outputs_test).detach().numpy()
    preds_bin = np.zeros_like(y_scores_test)

    for i in range(5):
        preds_bin[:, i] = (y_scores_test[:, i] > optimal_thresholds[i]).astype(int)
    f1_micro = f1_score(y_test, preds_bin, average='micro')
    f1_macro = f1_score(y_test, preds_bin, average='macro')
print(f"RF Results (%)")
print(f"F1 Micro: {f1_micro:.4f}")
print(f"F1 Macro: {f1_macro:.4f}")

"""DT"""

R = 306
# Set seeds
np.random.seed(R)
torch.manual_seed(R)

# Load and preprocess data
data = pd.read_csv('/content/project/project/dataset/Fused-Arabic/dataset_Combined_Arabic.csv')
data.dropna(inplace=True)
traits = ['E', 'N', 'A', 'C', 'O']
labels = data[['E', 'N', 'A', 'C', 'O']]
embeddings = joblib.load('/content/project/project/dataset/Fused-Arabic/bert_embeddings_Merge_arabic.pkl')
X = np.array(embeddings)
y = labels.to_numpy()
y = np.where(y == -1, 0, y)

# Split
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=R1)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=R1)

# Convert tensors for gating
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32)
y_val_tensor = torch.tensor(y_val, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32)

# Define Expert class using DecisionTreeClassifier
class DecisionTreeExpert:
    def __init__(self, criterion='entropy', splitter='best', max_depth=3, min_samples_split=3, min_samples_leaf=3,
                 min_weight_fraction_leaf=0.0, max_features='log2', random_state=44, max_leaf_nodes=None,
                 min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.001):
        self.model = DecisionTreeClassifier(
            criterion=criterion,
            splitter=splitter,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            random_state=random_state,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            class_weight=class_weight,
            ccp_alpha=ccp_alpha
        )

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict_proba(self, X):
        if hasattr(self.model, 'predict_proba'):
            return self.model.predict_proba(X)[:, 1]
        else:
            return self.model.predict(X)

# MGMOE model using PyTorch gates and DecisionTree experts
class MultiGateMixtureOfExperts:
    def __init__(self, input_dim, num_tasks, num_experts):
        self.num_tasks = num_tasks
        self.num_experts = num_experts
        self.experts = [DecisionTreeExpert() for _ in range(num_experts)]
        self.gates = torch.nn.ParameterList([
            torch.nn.Parameter(torch.randn(input_dim, num_experts, requires_grad=True))
            for _ in range(num_tasks)
        ])
        self.optimizer = torch.optim.Adam(self.gates.parameters(), lr=0.01)

    def train_experts(self, X, y):
        for i in range(self.num_experts):
            self.experts[i].train(X, y[:, i])

    def forward(self, X):
        if isinstance(X, np.ndarray):
            X = torch.tensor(X, dtype=torch.float32)

        with torch.no_grad():
            expert_outputs = [torch.tensor(expert.predict_proba(X.numpy()), dtype=torch.float32)
                              for expert in self.experts]
            expert_outputs = torch.stack(expert_outputs, dim=-1)  # (batch, experts)

        outputs = []
        for i, gate in enumerate(self.gates):
            gate_scores = F.softmax(torch.matmul(X, gate), dim=-1)  # (batch, experts)
            task_output = torch.sum(expert_outputs * gate_scores, dim=-1)  # (batch,)
            outputs.append(task_output)

        return torch.stack(outputs, dim=-1)  # (batch, tasks)

    def train_gates(self, X_tensor, y_tensor, epochs=30, batch_size=64):
        global L_label
        for epoch in range(epochs):
            epoch_loss = 0
            for i in range(0, len(X_tensor), batch_size):
                batch_x = X_tensor[i:i + batch_size]
                batch_y = y_tensor[i:i + batch_size]
                preds = self.forward(batch_x)
                loss_val = F.mse_loss(preds, batch_y)
                self.optimizer.zero_grad()
                loss_val.backward()
                self.optimizer.step()
                epoch_loss += loss_val.item()
            L_label = epoch_loss / (len(X_tensor) // batch_size)
            # print(f"Epoch {epoch + 1}/{epochs}, Loss: {L_label:.4f}")

# Hyperparameters
input_dim = X_train.shape[1]
num_tasks = 5
num_experts = 3

# Initialize model
model = MultiGateMixtureOfExperts(input_dim, num_tasks, num_experts)

# Train experts (on each task)
model.train_experts(X_train, y_train)

# Train gates using PyTorch
model.train_gates(X_train_tensor, y_train_tensor, epochs=30, batch_size=64)

# Evaluate
model_output = model.forward(X_test_tensor)
y_pred = model_output.detach().numpy()
y_pred_labels = (y_pred > 0.5).astype(int)

# Metrics
lambda_f = accuracy_score(y_test, y_pred_labels)
# ----------------------------------
# SHAP Integration
# ----------------------------------
def model_predict(X_np):
    return model.forward(X_np).detach().numpy()

X_sample = X_train[:50]
X_explain = X_test[:10]
explainer = shap.KernelExplainer(model_predict, X_sample)
shap_values = explainer.shap_values(X_explain)
# shap.summary_plot(shap_values, X_explain)

# Wasserstein Distance Loss
def wasserstein_loss(shap_values, reference_values):
    shap_values = np.array(shap_values)
    reference_values = np.array(reference_values)
    return np.mean([
        wasserstein_distance(shap_values[i].flatten(), reference_values[i].flatten())
        for i in range(shap_values.shape[0])
    ])

L_exp = wasserstein_loss(shap_values, np.mean(shap_values, axis=1))
L_joint = (1 - lambda_f) * L_label + lambda_f * L_exp

# ===========================
# DecisionTree Expert Class
# ===========================
class DecisionTreeExpert:
    def __init__(self, criterion='entropy', splitter='best', max_depth=3, min_samples_split=3, min_samples_leaf=3,
                 min_weight_fraction_leaf=0.0, max_features='log2', random_state=42, max_leaf_nodes=None,
                 min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.001):
        self.model = DecisionTreeClassifier(
            criterion=criterion,
            splitter=splitter,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            random_state=random_state,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            class_weight=class_weight,
            ccp_alpha=ccp_alpha
        )

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict_proba(self, X):
        if hasattr(self.model, 'predict_proba'):
            return self.model.predict_proba(X)[:, 1]
        else:
            return self.model.predict(X)

# Train multiple Decision Tree experts
num_experts = 3
rf_experts = [[DecisionTreeExpert() for _ in range(num_experts)] for _ in range(5)]

# Train each expert on full training data
for task_idx in range(5):
    for expert_idx in range(num_experts):
        rf_experts[task_idx][expert_idx].train(X_train, y_train[:, task_idx])

# =============================
# Gating Network (PyTorch)
# =============================
class MGMOE_GatingOnly(nn.Module):
    def __init__(self, input_dim, num_tasks, num_experts):
        super(MGMOE_GatingOnly, self).__init__()
        self.num_tasks = num_tasks
        self.num_experts = num_experts
        self.gates = nn.ParameterList([
            nn.Parameter(torch.randn(input_dim, num_experts)) for _ in range(num_tasks)
        ])

    def forward(self, x, expert_outputs):
        batch_size = x.shape[0]
        task_outputs = []
        for t in range(self.num_tasks):
            gate_logits = x @ self.gates[t]
            gate_weights = torch.softmax(gate_logits, dim=-1)
            weighted_output = (expert_outputs[:, t, :] * gate_weights).sum(dim=-1)
            task_outputs.append(weighted_output)
        return torch.stack(task_outputs, dim=-1)

# ===============================
# Focal Loss for Multilabel Task
# ===============================
class FocalLoss(nn.Module):
    def __init__(self, gamma=2.0, alpha=0.25):
        super(FocalLoss, self).__init__()
        self.gamma = gamma
        self.alpha = alpha

    def forward(self, inputs, targets):
        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        at = self.alpha * targets + (1 - self.alpha) * (1 - targets)
        pt = torch.exp(-BCE_loss)
        loss = at * (1 - pt) ** self.gamma * BCE_loss
        return loss.mean()

# ========================================
# Prepare Expert Outputs for Gating Net
# ========================================
def get_expert_outputs(X):
    outputs = np.zeros((X.shape[0], 5, num_experts))
    for task_idx in range(5):
        for expert_idx in range(num_experts):
            prob = rf_experts[task_idx][expert_idx].predict_proba(X)
            outputs[:, task_idx, expert_idx] = prob
    return torch.tensor(outputs, dtype=torch.float32)
# Set random seeds
np.random.seed(R)
torch.manual_seed(R)

# Load and preprocess data
data = pd.read_csv('/content/project/project/dataset/Fused-Arabic/dataset_Combined_Arabic.csv')
data.dropna(inplace=True)
texts = data['text']
labels = data[['E', 'N', 'A', 'C', 'O']]

# Load BERT embeddings
embeddings = joblib.load('/content/project/project/dataset/Fused-Arabic/bert_embeddings_Merge_arabic.pkl')
X = np.array(embeddings)
y = np.column_stack([labels[col] for col in ['E', 'N', 'A', 'C', 'O']])
y = np.clip(y, 0, 1).astype(np.float32)

# Train/val/test split
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=R)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=R)

# Convert val/test to torch tensors
X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_val_tensor = torch.tensor(y_val, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32)
# Get expert outputs for train/val
expert_outputs_train = get_expert_outputs(X_train)
expert_outputs_val = get_expert_outputs(X_val)

X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32)

# ==========================================
# Training Gating Network
# ==========================================
input_dim = X.shape[1]
model = MGMOE_GatingOnly(input_dim, num_tasks=5, num_experts=num_experts)
optimizer = optim.Adam(model.parameters(), lr=0.01)
scheduler = OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(X_train) // 32, epochs=30)
loss_fn = FocalLoss()

# EarlyStopping class
class EarlyStopping:
    def __init__(self, patience=5, verbose=True, delta=0.0001):
        self.patience = patience
        self.verbose = verbose
        self.delta = delta
        self.best_score = None
        self.epochs_no_improve = 0
        self.early_stop = False

    def __call__(self, val_loss, model):
        score = -val_loss
        if self.best_score is None:
            self.best_score = score
        elif score < self.best_score + self.delta:
            self.epochs_no_improve += 1
            if self.epochs_no_improve >= self.patience:
                if self.verbose:
                    print("Early stopping triggered.")
                self.early_stop = True
        else:
            self.best_score = score
            self.epochs_no_improve = 0

# Training loop
def train_model(model, X_train, expert_outputs_train, y_train, X_val, expert_outputs_val, y_val, loss_fn, optimizer, scheduler, epochs=30, batch_size=32):
    early_stopping = EarlyStopping()
    for epoch in range(epochs):
        model.train()
        permutation = torch.randperm(X_train.size(0))
        epoch_loss = 0
        for i in range(0, X_train.size(0), batch_size):
            indices = permutation[i:i + batch_size]
            batch_x = X_train[indices]
            batch_y = y_train[indices]
            batch_exp = expert_outputs_train[indices]

            optimizer.zero_grad()
            outputs = model(batch_x, batch_exp)
            loss = loss_fn(outputs, batch_y)
            loss.backward()
            optimizer.step()
            scheduler.step()
            epoch_loss += loss.item()

        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val, expert_outputs_val)
            val_loss = loss_fn(val_outputs, y_val)
        # print(f"Epoch {epoch+1}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}")
        early_stopping(val_loss.item(), model)
        if early_stopping.early_stop:
            break

train_model(model, X_train_tensor, expert_outputs_train, y_train_tensor,
            X_val_tensor, expert_outputs_val, y_val_tensor,
            loss_fn, optimizer, scheduler)

# ==========================================
# Evaluation with Optimal Thresholds
# ==========================================
def calculate_optimal_thresholds(y_true, y_scores):
    thresholds = []
    for i in range(y_true.shape[1]):
        precision, recall, thresh = precision_recall_curve(y_true[:, i], y_scores[:, i])
        f1 = 2 * (precision * recall) / (precision + recall + 1e-8)
        best_thresh = thresh[np.argmax(f1)] if len(thresh) > 0 else 0.5
        thresholds.append(best_thresh)
    return thresholds

expert_outputs_test = get_expert_outputs(X_test)
optimal_thresholds = calculate_optimal_thresholds(y_val, model(X_val_tensor, expert_outputs_val).detach().numpy())
model.eval()
with torch.no_grad():
    y_scores_test = model(X_test_tensor, expert_outputs_test).detach().numpy()
    preds_bin = np.zeros_like(y_scores_test)

    for i in range(5):
        preds_bin[:, i] = (y_scores_test[:, i] > optimal_thresholds[i]).astype(int)
    f1_micro = f1_score(y_test, preds_bin, average='micro')
    f1_macro = f1_score(y_test, preds_bin, average='macro')
print(f"DT Results (%)")
print(f"F1 Micro: {f1_micro:.4f}")
print(f"F1 Macro: {f1_macro:.4f}")

"""SVC"""

R = 174
# Set seeds
np.random.seed(R)
torch.manual_seed(R)

# Load and preprocess data
data = pd.read_csv('/content/project/project/dataset/Fused-Arabic/dataset_Combined_Arabic.csv')
data.dropna(inplace=True)

traits = ['E', 'N', 'A', 'C', 'O']
labels = data[['E', 'N', 'A', 'C', 'O']]
embeddings = joblib.load('/content/project/project/dataset/Fused-Arabic/bert_embeddings_Merge_arabic.pkl')
X = np.array(embeddings)
y = labels.to_numpy()
y = np.where(y == -1, 0, y)

# Split
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=R1)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=R1)

# Convert tensors for gating
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32)
y_val_tensor = torch.tensor(y_val, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32)

# Define Expert class using SVC
class SVCExpert:
    def __init__(self, C=200, kernel='linear', degree=3, gamma='scale', tol=0.0001,
                 class_weight=None, random_state=42, max_iter=-1):
        self.model = SVC(
            C=C,
            kernel=kernel,
            degree=degree,
            gamma=gamma,
            tol=tol,
            class_weight=class_weight,
            random_state=random_state,
            max_iter=max_iter,
            probability=True
        )

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict_proba(self, X):
        return self.model.predict_proba(X)[:, 1]  # Return positive class probabilities

# MGMOE model using PyTorch gates and SVC experts
class MultiGateMixtureOfExperts:
    def __init__(self, input_dim, num_tasks, num_experts):
        self.num_tasks = num_tasks
        self.num_experts = num_experts
        self.experts = [SVCExpert() for _ in range(num_experts)]
        self.gates = torch.nn.ParameterList([
            torch.nn.Parameter(torch.randn(input_dim, num_experts, requires_grad=True))
            for _ in range(num_tasks)
        ])
        self.optimizer = torch.optim.Adam(self.gates.parameters(), lr=0.01)

    def train_experts(self, X, y):
        for i in range(self.num_experts):
            self.experts[i].train(X, y[:, i])

    def forward(self, X):
        if isinstance(X, np.ndarray):
            X = torch.tensor(X, dtype=torch.float32)

        with torch.no_grad():
            expert_outputs = [torch.tensor(expert.predict_proba(X.numpy()), dtype=torch.float32)
                              for expert in self.experts]
            expert_outputs = torch.stack(expert_outputs, dim=-1)  # (batch, experts)

        outputs = []
        for i, gate in enumerate(self.gates):
            gate_scores = F.softmax(torch.matmul(X, gate), dim=-1)  # (batch, experts)
            task_output = torch.sum(expert_outputs * gate_scores, dim=-1)  # (batch,)
            outputs.append(task_output)

        return torch.stack(outputs, dim=-1)  # (batch, tasks)

    def train_gates(self, X_tensor, y_tensor, epochs=30, batch_size=64):
        global L_label
        for epoch in range(epochs):
            epoch_loss = 0
            for i in range(0, len(X_tensor), batch_size):
                batch_x = X_tensor[i:i + batch_size]
                batch_y = y_tensor[i:i + batch_size]
                preds = self.forward(batch_x)
                loss_val = F.mse_loss(preds, batch_y)
                self.optimizer.zero_grad()
                loss_val.backward()
                self.optimizer.step()
                epoch_loss += loss_val.item()
            L_label = epoch_loss / (len(X_tensor) // batch_size)
            # print(f"Epoch {epoch + 1}/{epochs}, Loss: {L_label:.4f}")

# Hyperparameters
input_dim = X_train.shape[1]
num_tasks = 5
num_experts = 3

# Initialize model
model = MultiGateMixtureOfExperts(input_dim, num_tasks, num_experts)

# Train experts (on each task)
model.train_experts(X_train, y_train)

# Train gates using PyTorch
model.train_gates(X_train_tensor, y_train_tensor, epochs=30, batch_size=64)

# Evaluate
model_output = model.forward(X_test_tensor)
y_pred = model_output.detach().numpy()
y_pred_labels = (y_pred > 0.5).astype(int)

# Metrics
lambda_f = accuracy_score(y_test, y_pred_labels)

# SHAP Integration
def model_predict(X_np):
    return model.forward(X_np).detach().numpy()
X_sample = X_train[:50]
X_explain = X_test[:10]
explainer = shap.KernelExplainer(model_predict, X_sample)
shap_values = explainer.shap_values(X_explain)
# shap.summary_plot(shap_values, X_explain)

# Wasserstein Distance Loss
def wasserstein_loss(shap_values, reference_values):
    shap_values = np.array(shap_values)
    reference_values = np.array(reference_values)
    return np.mean([
        wasserstein_distance(shap_values[i].flatten(), reference_values[i].flatten())
        for i in range(shap_values.shape[0])
    ])

L_exp = wasserstein_loss(shap_values, np.mean(shap_values, axis=1))
L_joint = (1 - lambda_f) * L_label + lambda_f * L_exp

# ===========================
# SVC Expert Class
# ===========================
class SVCExpert:
    def __init__(self, C=50, kernel='linear', degree=2, gamma='scale', tol=0.01,
                 class_weight=None, random_state=42, max_iter=-1):
        self.model = SVC(
            C=C,
            kernel=kernel,
            degree=degree,
            gamma=gamma,
            tol=tol,
            class_weight=class_weight,
            random_state=random_state,
            max_iter=max_iter,
            probability=True
        )

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict_proba(self, X):
        return self.model.predict_proba(X)[:, 1]  # Return positive class probabilities
# R = 174
# # Set random seeds
# np.random.seed(R)
# torch.manual_seed(R)
# =============================
# Gating Network (PyTorch)
# =============================
class MGMOE_GatingOnly(nn.Module):
    def __init__(self, input_dim, num_tasks, num_experts):
        super(MGMOE_GatingOnly, self).__init__()
        self.num_tasks = num_tasks
        self.num_experts = num_experts
        self.gates = nn.ParameterList([
            nn.Parameter(torch.randn(input_dim, num_experts)) for _ in range(num_tasks)
        ])

    def forward(self, x, expert_outputs):  # expert_outputs: shape (batch, tasks, experts)
        batch_size = x.shape[0]
        task_outputs = []
        for t in range(self.num_tasks):
            gate_logits = x @ self.gates[t]  # (batch, experts)
            gate_weights = torch.softmax(gate_logits, dim=-1)
            weighted_output = (expert_outputs[:, t, :] * gate_weights).sum(dim=-1)
            task_outputs.append(weighted_output)
        return torch.stack(task_outputs, dim=-1)  # (batch, tasks)

# ===============================
# Focal Loss for Multilabel Task
# ===============================
class FocalLoss(nn.Module):
    def __init__(self, gamma=2.0, alpha=0.25):
        super(FocalLoss, self).__init__()
        self.gamma = gamma
        self.alpha = alpha

    def forward(self, inputs, targets):
        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        at = self.alpha * targets + (1 - self.alpha) * (1 - targets)
        pt = torch.exp(-BCE_loss)
        loss = at * (1 - pt) ** self.gamma * BCE_loss
        return loss.mean()

# ========================================
# Prepare Expert Outputs for Gating Net
# ========================================
def get_expert_outputs(X):
    outputs = np.zeros((X.shape[0], 5, num_experts))
    for task_idx in range(5):
        for expert_idx in range(num_experts):
            prob = svc_experts[task_idx][expert_idx].predict_proba(X)
            outputs[:, task_idx, expert_idx] = prob
    return torch.tensor(outputs, dtype=torch.float32)
# Load and preprocess data
data = pd.read_csv('/content/project/project/dataset/Fused-Arabic/dataset_Combined_Arabic.csv')
data.dropna(inplace=True)
texts = data['text']
labels = data[['E', 'N', 'A', 'C', 'O']]

# Load BERT embeddings
embeddings = joblib.load('/content/project/project/dataset/Fused-Arabic/bert_embeddings_Merge_arabic.pkl')
X = np.array(embeddings)
y = np.column_stack([labels[col] for col in ['E', 'N', 'A', 'C', 'O']])
y = np.clip(y, 0, 1).astype(np.float32)

# Train/val/test split
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=R)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=R)

# Convert val/test to torch tensors
X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_val_tensor = torch.tensor(y_val, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32)
# Train multiple SVC experts
num_experts = 3
svc_experts = [[SVCExpert() for _ in range(num_experts)] for _ in range(5)]  # 5 tasks × N experts

# Train each expert on full training data
for task_idx in range(5):
    for expert_idx in range(num_experts):
        svc_experts[task_idx][expert_idx].train(X_train, y_train[:, task_idx])
# Get expert outputs for train/val
expert_outputs_train = get_expert_outputs(X_train)
expert_outputs_val = get_expert_outputs(X_val)

# Convert X_train to tensor
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32)

# ==========================================
# Training Gating Network (Only This Trains)
# ==========================================
input_dim = X.shape[1]
model = MGMOE_GatingOnly(input_dim, num_tasks=5, num_experts=num_experts)
optimizer = optim.Adam(model.parameters(), lr=0.01)
scheduler = OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(X_train) // 32, epochs=30)
loss_fn = FocalLoss()

# EarlyStopping class
class EarlyStopping:
    def __init__(self, patience=5, verbose=True, delta=0.0001):
        self.patience = patience
        self.verbose = verbose
        self.delta = delta
        self.best_score = None
        self.epochs_no_improve = 0
        self.early_stop = False

    def __call__(self, val_loss, model):
        score = -val_loss
        if self.best_score is None:
            self.best_score = score
        elif score < self.best_score + self.delta:
            self.epochs_no_improve += 1
            if self.epochs_no_improve >= self.patience:
                if self.verbose:
                    print("Early stopping triggered.")
                self.early_stop = True
        else:
            self.best_score = score
            self.epochs_no_improve = 0

# Training loop
def train_model(model, X_train, expert_outputs_train, y_train, X_val, expert_outputs_val, y_val, loss_fn, optimizer, scheduler, epochs=30, batch_size=32):
    early_stopping = EarlyStopping()
    for epoch in range(epochs):
        model.train()
        permutation = torch.randperm(X_train.size(0))
        epoch_loss = L_joint
        for i in range(0, X_train.size(0), batch_size):
            indices = permutation[i:i + batch_size]
            batch_x = X_train[indices]
            batch_y = y_train[indices]
            batch_exp = expert_outputs_train[indices]

            optimizer.zero_grad()
            outputs = model(batch_x, batch_exp)
            loss = loss_fn(outputs, batch_y)
            loss.backward()
            optimizer.step()
            scheduler.step()
            epoch_loss += loss.item()

        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val, expert_outputs_val)
            val_loss = loss_fn(val_outputs, y_val)
        # print(f"Epoch {epoch+1}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}")
        early_stopping(val_loss.item(), model)
        if early_stopping.early_stop:
            break

train_model(model, X_train_tensor, expert_outputs_train, y_train_tensor,
            X_val_tensor, expert_outputs_val, y_val_tensor,
            loss_fn, optimizer, scheduler)

# ==========================================
# Evaluation with Optimal Thresholds
# ==========================================
def calculate_optimal_thresholds(y_true, y_scores):
    thresholds = []
    for i in range(y_true.shape[1]):
        precision, recall, thresh = precision_recall_curve(y_true[:, i], y_scores[:, i])
        f1 = 2 * (precision * recall) / (precision + recall + 1e-8)
        best_thresh = thresh[np.argmax(f1)] if len(thresh) > 0 else 0.5
        thresholds.append(best_thresh)
    return thresholds

# Evaluate on test
expert_outputs_test = get_expert_outputs(X_test)
optimal_thresholds = calculate_optimal_thresholds(y_val, model(X_val_tensor, expert_outputs_val).detach().numpy())
model.eval()
with torch.no_grad():
    y_scores_test = model(X_test_tensor, expert_outputs_test).detach().numpy()
    preds_bin = np.zeros_like(y_scores_test)

    for i in range(5):
        preds_bin[:, i] = (y_scores_test[:, i] > optimal_thresholds[i]).astype(int)
    f1_micro = f1_score(y_test, preds_bin, average='micro')
    f1_macro = f1_score(y_test, preds_bin, average='macro')

print(f"SVC Results (%)")
print(f"F1 Micro: {f1_micro:.4f}")
print(f"F1 Macro: {f1_macro:.4f}")